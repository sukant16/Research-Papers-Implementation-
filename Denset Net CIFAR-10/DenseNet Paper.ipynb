{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K70hAckqg0EA",
    "outputId": "8269f97e-e670-4849-e57e-a8b3fd0754a9"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/\n",
    "# !pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RGwh6HCBwFq8",
    "outputId": "991fbc54-2aa6-4fa3-8175-160598614985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/sukant/Moz/DS/DL/CIFAR10'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "57db8d56-5e4f-4457-9ac0-09e3274cd40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 90s 1us/step\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjTxZgPMtNbI"
   },
   "outputs": [],
   "source": [
    "# standardizing the images\n",
    "x_train_mean = x_train.mean(axis =0)\n",
    "x_train_std = x_train.std(axis=0)\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_train_mean)/x_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32, 3), (32, 32, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_mean.shape, x_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the standardizing arrays to standardize any test image further\n",
    "with open(\"standardizing_arrays.pkl\", 'wb') as f:\n",
    "    pickle.dump([x_train_mean, x_train_std], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZtqrUT61I8L"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "l = 16\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIQWk1laLx6k"
   },
   "outputs": [],
   "source": [
    "# convolution\n",
    "def conv_layer(input, num_filter, dropout_rate, l2_reg, kernel_size =3):\n",
    "  BatchNorm = BatchNormalization(gamma_regularizer=l2(l2_reg), beta_regularizer=l2(l2_reg))(input)\n",
    "  relu = Activation('relu')(BatchNorm)\n",
    "  conv = Conv2D(num_filter, kernel_size, padding = 'same', kernel_regularizer = l2(l2_reg), use_bias = False)(relu)\n",
    "  if dropout_rate>0:\n",
    "    conv = Dropout(dropout_rate)(conv)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNDIA5ZyLGYR"
   },
   "outputs": [],
   "source": [
    "# bottleneck layer\n",
    "def bottleneck_layer(input, num_filter, dropout_rate, l2_reg, kernel_size=3):\n",
    "  conv1 = conv_layer(input, num_filter*4, dropout_rate, l2_reg, kernel_size=1)\n",
    "  conv3 = conv_layer(conv1, num_filter, dropout_rate, l2_reg, kernel_size=kernel_size)\n",
    "  return conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter, dropout_rate, l2_reg, bottleneck=False, kernel_size=3):\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        if bottleneck:\n",
    "          output = bottleneck_layer(temp, num_filter, dropout_rate, l2_reg, kernel_size=kernel_size)\n",
    "        else:\n",
    "          output = conv_layer(temp, num_filter, dropout_rate, l2_reg, kernel_size=3)\n",
    "    \n",
    "        concat = Concatenate(axis=-1)([temp,output])\n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "# Transition Block\n",
    "def add_transition(input, num_filter, dropout_rate, l2_reg):\n",
    "    output = conv_layer(input, num_filter, dropout_rate, l2_reg, kernel_size=1)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(output)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "# Output layer\n",
    "def output_layer(input, l2_reg):\n",
    "    BatchNorm = BatchNormalization(gamma_regularizer=l2(l2_reg), beta_regularizer=l2(l2_reg))(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_blocks = 3\n",
    "num_channels = 32\n",
    "l2_reg = 1e-4\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "out = Conv2D(num_channels, (3,3), use_bias=False, kernel_regularizer = l2(l2_reg), padding='same')(input)\n",
    "\n",
    "for i in range(num_blocks-1):\n",
    "  out = add_denseblock(out, num_filter=num_filter, dropout_rate=dropout_rate, l2_reg= l2_reg, bottleneck=True)\n",
    "  num_channels += l*num_filter\n",
    "  num_channels = int(num_channels*compression)\n",
    "  out = add_transition(out, num_filter=num_channels, dropout_rate=dropout_rate, l2_reg= l2_reg)\n",
    "\n",
    "out = add_denseblock(out, num_filter=num_filter, dropout_rate=dropout_rate, l2_reg= l2_reg, bottleneck=True)\n",
    "\n",
    "output = output_layer(out, l2_reg = 1e-4,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16027
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "aa8e96f3-2474-43c0-bf8a-78d68b123ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 48)   1536        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 48)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 12)   5184        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 44)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 44)   176         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 44)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 48)   2112        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 48)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 12)   5184        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 56)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 56)   224         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 56)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 48)   2688        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 48)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 12)   5184        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 68)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 68)   272         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 68)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 48)   3264        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 48)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 48)   192         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 48)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 12)   5184        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 80)   0           concatenate_3[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 80)   320         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 80)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 48)   3840        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 48)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 48)   192         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 48)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 12)   5184        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 92)   0           concatenate_4[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 92)   368         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 92)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 48)   4416        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 48)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 48)   192         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 48)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 12)   5184        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 104)  0           concatenate_5[0][0]              \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 104)  416         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 104)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 48)   4992        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 48)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 48)   192         dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 12)   5184        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 116)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 116)  464         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 116)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 48)   5568        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 48)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 48)   192         dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 48)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 12)   5184        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 32, 32, 12)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 128)  0           concatenate_7[0][0]              \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 48)   6144        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32, 32, 48)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 48)   192         dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 48)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 12)   5184        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32, 32, 12)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 140)  0           concatenate_8[0][0]              \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 140)  560         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 140)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 48)   6720        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 32, 32, 48)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 48)   192         dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 12)   5184        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32, 32, 12)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 152)  0           concatenate_9[0][0]              \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 152)  608         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 152)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 48)   7296        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32, 32, 48)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 48)   192         dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 32, 48)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 12)   5184        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32, 32, 12)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 164)  0           concatenate_10[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 164)  656         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 164)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 48)   7872        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32, 32, 48)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 48)   192         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 48)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 12)   5184        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32, 32, 12)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 176)  0           concatenate_11[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 176)  704         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 176)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 48)   8448        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 32, 32, 48)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 48)   192         dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 48)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 12)   5184        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32, 32, 12)   0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 188)  0           concatenate_12[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 188)  752         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 188)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 48)   9024        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 32, 32, 48)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 48)   192         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 48)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 12)   5184        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32, 32, 12)   0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 200)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 32, 200)  800         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 32, 200)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 48)   9600        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 32, 32, 48)   0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 32, 48)   192         dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 32, 48)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 12)   5184        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 32, 32, 12)   0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 212)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 212)  848         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 212)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 48)   10176       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 32, 32, 48)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 48)   192         dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 48)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 12)   5184        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 32, 32, 12)   0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 32, 32, 224)  0           concatenate_15[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 32, 224)  896         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 224)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 112)  25088       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 32, 32, 112)  0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 112)  0           dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 112)  448         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 112)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 48)   5376        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 16, 16, 48)   0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 48)   192         dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 48)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 12)   5184        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 16, 16, 12)   0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 124)  0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 124)  496         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 124)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 48)   5952        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 16, 16, 48)   0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 48)   192         dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 48)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 12)   5184        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 16, 16, 12)   0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 136)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 136)  544         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 136)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 48)   6528        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 16, 16, 48)   0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 48)   192         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 48)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 12)   5184        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 16, 16, 12)   0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 148)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 148)  592         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 148)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 48)   7104        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 16, 16, 48)   0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 48)   192         dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 48)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 12)   5184        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 16, 16, 12)   0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 160)  0           concatenate_19[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 160)  640         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 48)   7680        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 16, 16, 48)   0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 48)   192         dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 48)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 12)   5184        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 16, 16, 12)   0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 172)  0           concatenate_20[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 172)  688         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 172)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 16, 16, 48)   8256        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 16, 16, 48)   0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 48)   192         dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 48)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 16, 16, 12)   5184        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 16, 16, 12)   0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 184)  0           concatenate_21[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 184)  736         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 184)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 16, 16, 48)   8832        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 16, 16, 48)   0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 16, 16, 48)   192         dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 48)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 16, 16, 12)   5184        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 16, 16, 12)   0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 196)  0           concatenate_22[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 16, 16, 196)  784         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 196)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 16, 16, 48)   9408        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 16, 16, 48)   0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 16, 16, 48)   192         dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 48)   0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 16, 16, 12)   5184        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 16, 16, 12)   0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 208)  0           concatenate_23[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 16, 16, 208)  832         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 16, 16, 208)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 16, 16, 48)   9984        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 16, 16, 48)   0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 16, 16, 48)   192         dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16, 16, 48)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 16, 16, 12)   5184        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 16, 16, 12)   0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 16, 16, 220)  0           concatenate_24[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 16, 16, 220)  880         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 16, 220)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 16, 16, 48)   10560       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 16, 16, 48)   0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 16, 16, 48)   192         dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 16, 48)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 12)   5184        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 16, 16, 12)   0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 232)  0           concatenate_25[0][0]             \n",
      "                                                                 dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 232)  928         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 232)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 48)   11136       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 16, 16, 48)   0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 48)   192         dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 48)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 12)   5184        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 16, 16, 12)   0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 244)  0           concatenate_26[0][0]             \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 244)  976         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 244)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 48)   11712       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 16, 16, 48)   0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 48)   192         dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 48)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 12)   5184        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 16, 16, 12)   0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 16, 16, 256)  0           concatenate_27[0][0]             \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 256)  1024        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 256)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 48)   12288       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 16, 16, 48)   0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 48)   192         dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 48)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 12)   5184        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 16, 16, 12)   0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 16, 16, 268)  0           concatenate_28[0][0]             \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 268)  1072        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 268)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 48)   12864       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 16, 16, 48)   0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 48)   192         dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 48)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 12)   5184        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 16, 16, 12)   0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 16, 16, 280)  0           concatenate_29[0][0]             \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 280)  1120        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 280)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 48)   13440       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 16, 16, 48)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 48)   192         dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 48)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 12)   5184        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 16, 16, 12)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 292)  0           concatenate_30[0][0]             \n",
      "                                                                 dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 292)  1168        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 292)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 48)   14016       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 16, 16, 48)   0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 48)   192         dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 48)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 12)   5184        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 16, 16, 12)   0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 304)  0           concatenate_31[0][0]             \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 304)  1216        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 304)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 152)  46208       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 16, 16, 152)  0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 152)    0           dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 152)    608         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 8, 152)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 8, 48)     7296        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 8, 8, 48)     0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 48)     192         dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 8, 48)     0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 12)     5184        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 8, 8, 12)     0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 164)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 164)    656         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 8, 164)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 48)     7872        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 8, 8, 48)     0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 48)     192         dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 8, 48)     0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 12)     5184        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 8, 8, 12)     0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 176)    0           concatenate_33[0][0]             \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 176)    704         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 8, 176)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 48)     8448        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 8, 8, 48)     0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 48)     192         dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 48)     0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 12)     5184        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 8, 8, 12)     0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 188)    0           concatenate_34[0][0]             \n",
      "                                                                 dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 188)    752         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 188)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 48)     9024        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 8, 8, 48)     0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 48)     192         dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 48)     0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 8, 12)     5184        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 8, 8, 12)     0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 200)    0           concatenate_35[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 200)    800         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 200)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 48)     9600        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 8, 8, 48)     0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 48)     192         dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 48)     0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 12)     5184        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 8, 8, 12)     0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 8, 8, 212)    0           concatenate_36[0][0]             \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 212)    848         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 212)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 48)     10176       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 8, 8, 48)     0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 48)     192         dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 48)     0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 12)     5184        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 8, 8, 12)     0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 8, 8, 224)    0           concatenate_37[0][0]             \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 224)    896         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 224)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 48)     10752       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 8, 8, 48)     0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 48)     192         dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 48)     0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 12)     5184        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 8, 8, 12)     0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 8, 8, 236)    0           concatenate_38[0][0]             \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 236)    944         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 236)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 48)     11328       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 8, 8, 48)     0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 48)     192         dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 48)     0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 12)     5184        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 8, 8, 12)     0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 8, 8, 248)    0           concatenate_39[0][0]             \n",
      "                                                                 dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 248)    992         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 248)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 48)     11904       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 8, 8, 48)     0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 48)     192         dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 48)     0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 12)     5184        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 8, 8, 12)     0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 8, 8, 260)    0           concatenate_40[0][0]             \n",
      "                                                                 dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 260)    1040        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 260)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 48)     12480       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 8, 8, 48)     0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 48)     192         dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 48)     0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 12)     5184        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 8, 8, 12)     0           conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 8, 8, 272)    0           concatenate_41[0][0]             \n",
      "                                                                 dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 272)    1088        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 272)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 48)     13056       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 8, 8, 48)     0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 48)     192         dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 48)     0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 12)     5184        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 8, 8, 12)     0           conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 284)    0           concatenate_42[0][0]             \n",
      "                                                                 dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 284)    1136        concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 284)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 48)     13632       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 8, 8, 48)     0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 48)     192         dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 48)     0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 12)     5184        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 8, 8, 12)     0           conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 296)    0           concatenate_43[0][0]             \n",
      "                                                                 dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 296)    1184        concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 296)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 48)     14208       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 8, 8, 48)     0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 48)     192         dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 48)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 12)     5184        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 8, 8, 12)     0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 308)    0           concatenate_44[0][0]             \n",
      "                                                                 dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 308)    1232        concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 308)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 48)     14784       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 8, 8, 48)     0           conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 48)     192         dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 48)     0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 8, 8, 12)     5184        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 8, 8, 12)     0           conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 320)    0           concatenate_45[0][0]             \n",
      "                                                                 dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 320)    1280        concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 8, 8, 320)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 8, 8, 48)     15360       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 8, 8, 48)     0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 48)     192         dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 8, 8, 48)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 12)     5184        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 8, 8, 12)     0           conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 8, 8, 332)    0           concatenate_46[0][0]             \n",
      "                                                                 dropout_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 332)    1328        concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 8, 8, 332)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 48)     15936       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 8, 8, 48)     0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 8, 8, 48)     192         dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 8, 8, 48)     0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 12)     5184        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 8, 8, 12)     0           conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 8, 8, 344)    0           concatenate_47[0][0]             \n",
      "                                                                 dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 8, 8, 344)    1376        concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 8, 8, 344)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 344)    0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5504)         0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           55050       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 859,658\n",
      "Trainable params: 835,194\n",
      "Non-trainable params: 24,464\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQKNN4sa43LN"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLpqOffGe6BW"
   },
   "outputs": [],
   "source": [
    "# Scheduling the learning rate to decrease by 0.1 after 50% of epochs and 75% of epochs\n",
    "from keras import backend as K\n",
    "def scheduler(epoch):\n",
    "  if epoch > 125 and epoch <= 188:\n",
    "    K.set_value(model.optimizer.lr, 0.01)\n",
    "  elif epoch >188:\n",
    "    K.set_value(model.optimizer.lr, 0.001)\n",
    "  return K.get_value(model.optimizer.lr)\n",
    "\n",
    "change_lr = callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISJ3DOHahQZ7"
   },
   "outputs": [],
   "source": [
    "# Creating checkpoints to save weights after every 10 epochs which will be saved only if the current epoch's validation accuracy is better than the last saved one\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath= 'DNST_Final_{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                                         monitor = 'val_acc',\n",
    "                                         mode = 'max',\n",
    "                                         save_best_only = True,\n",
    "                                         save_weights_only=True,\n",
    "                                         period =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pppaqk3P5gZ_"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.1, decay = 1e-6, momentum=0.9, nesterov=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1603
    },
    "colab_type": "code",
    "id": "X8JL32idH4Gz",
    "outputId": "147d6cbc-8dcc-490d-b0e5-984e9a3bc8c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=937, callbacks=[<keras.ca..., verbose=1, validation_data=(array([[[..., epochs=250)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "937/937 [==============================] - 210s 224ms/step - loss: 4.5757 - acc: 0.2810 - val_loss: 3.8231 - val_acc: 0.3775\n",
      "Epoch 2/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 3.5377 - acc: 0.3752 - val_loss: 3.2719 - val_acc: 0.3546\n",
      "Epoch 3/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 2.7819 - acc: 0.4335 - val_loss: 2.4716 - val_acc: 0.4714\n",
      "Epoch 4/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 2.2963 - acc: 0.4807 - val_loss: 1.9822 - val_acc: 0.5462\n",
      "Epoch 5/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 1.9497 - acc: 0.5167 - val_loss: 1.7685 - val_acc: 0.5660\n",
      "Epoch 6/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 1.7064 - acc: 0.5492 - val_loss: 1.5612 - val_acc: 0.5771\n",
      "Epoch 7/250\n",
      "937/937 [==============================] - 175s 186ms/step - loss: 1.5124 - acc: 0.5842 - val_loss: 1.7801 - val_acc: 0.5689\n",
      "Epoch 8/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 1.3687 - acc: 0.6149 - val_loss: 1.6714 - val_acc: 0.5715\n",
      "Epoch 9/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 1.2432 - acc: 0.6482 - val_loss: 1.2040 - val_acc: 0.6713\n",
      "Epoch 10/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 1.1654 - acc: 0.6661 - val_loss: 1.3342 - val_acc: 0.6505\n",
      "Epoch 11/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 1.1081 - acc: 0.6852 - val_loss: 1.0713 - val_acc: 0.7173\n",
      "Epoch 12/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 1.0622 - acc: 0.7007 - val_loss: 1.1754 - val_acc: 0.7062\n",
      "Epoch 13/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 1.0310 - acc: 0.7121 - val_loss: 1.2785 - val_acc: 0.6777\n",
      "Epoch 14/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 1.0065 - acc: 0.7222 - val_loss: 1.5186 - val_acc: 0.6643\n",
      "Epoch 15/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 1.0031 - acc: 0.7293 - val_loss: 1.2046 - val_acc: 0.6948\n",
      "Epoch 16/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9785 - acc: 0.7357 - val_loss: 1.9698 - val_acc: 0.6034\n",
      "Epoch 17/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9744 - acc: 0.7377 - val_loss: 0.9679 - val_acc: 0.7569\n",
      "Epoch 18/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9633 - acc: 0.7438 - val_loss: 1.2993 - val_acc: 0.6662\n",
      "Epoch 19/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9513 - acc: 0.7512 - val_loss: 1.2621 - val_acc: 0.6976\n",
      "Epoch 20/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9467 - acc: 0.7546 - val_loss: 1.0155 - val_acc: 0.7604\n",
      "Epoch 21/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9381 - acc: 0.7584 - val_loss: 1.1586 - val_acc: 0.7143\n",
      "Epoch 22/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.9411 - acc: 0.7579 - val_loss: 1.2067 - val_acc: 0.7228\n",
      "Epoch 23/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9325 - acc: 0.7628 - val_loss: 1.2765 - val_acc: 0.7006\n",
      "Epoch 24/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9261 - acc: 0.7658 - val_loss: 0.8743 - val_acc: 0.7884\n",
      "Epoch 25/250\n",
      "937/937 [==============================] - 173s 184ms/step - loss: 0.9228 - acc: 0.7691 - val_loss: 1.1146 - val_acc: 0.7485\n",
      "Epoch 26/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9250 - acc: 0.7688 - val_loss: 0.9848 - val_acc: 0.7517\n",
      "Epoch 27/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9106 - acc: 0.7738 - val_loss: 0.9669 - val_acc: 0.7695\n",
      "Epoch 28/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9174 - acc: 0.7734 - val_loss: 1.0701 - val_acc: 0.7461\n",
      "Epoch 29/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.9109 - acc: 0.7766 - val_loss: 1.0946 - val_acc: 0.7406\n",
      "Epoch 30/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9183 - acc: 0.7738 - val_loss: 1.2749 - val_acc: 0.7274\n",
      "Epoch 31/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.9199 - acc: 0.7753 - val_loss: 1.1651 - val_acc: 0.7347\n",
      "Epoch 32/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9076 - acc: 0.7795 - val_loss: 1.1033 - val_acc: 0.7499\n",
      "Epoch 33/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9054 - acc: 0.7814 - val_loss: 0.9441 - val_acc: 0.7837\n",
      "Epoch 34/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9013 - acc: 0.7821 - val_loss: 1.1652 - val_acc: 0.7123\n",
      "Epoch 35/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9024 - acc: 0.7830 - val_loss: 1.6128 - val_acc: 0.6627\n",
      "Epoch 36/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.9119 - acc: 0.7789 - val_loss: 1.0992 - val_acc: 0.7616\n",
      "Epoch 37/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.9015 - acc: 0.7850 - val_loss: 0.8844 - val_acc: 0.7975\n",
      "Epoch 38/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.9009 - acc: 0.7859 - val_loss: 1.0154 - val_acc: 0.7752\n",
      "Epoch 39/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8944 - acc: 0.7880 - val_loss: 1.1874 - val_acc: 0.7501\n",
      "Epoch 40/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.9026 - acc: 0.7852 - val_loss: 1.1593 - val_acc: 0.7312\n",
      "Epoch 41/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8936 - acc: 0.7884 - val_loss: 1.2751 - val_acc: 0.7207\n",
      "Epoch 42/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.8982 - acc: 0.7884 - val_loss: 1.0313 - val_acc: 0.7745\n",
      "Epoch 43/250\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.8932 - acc: 0.7906 - val_loss: 1.2202 - val_acc: 0.7467\n",
      "Epoch 44/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8984 - acc: 0.7891 - val_loss: 0.8893 - val_acc: 0.7990\n",
      "Epoch 45/250\n",
      "937/937 [==============================] - 173s 184ms/step - loss: 0.8950 - acc: 0.7910 - val_loss: 1.0306 - val_acc: 0.7750\n",
      "Epoch 46/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.8894 - acc: 0.7935 - val_loss: 0.9793 - val_acc: 0.7738\n",
      "Epoch 47/250\n",
      "937/937 [==============================] - 176s 187ms/step - loss: 0.8898 - acc: 0.7915 - val_loss: 1.0479 - val_acc: 0.7642\n",
      "Epoch 48/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.8876 - acc: 0.7919 - val_loss: 1.2675 - val_acc: 0.7219\n",
      "Epoch 49/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.8835 - acc: 0.7949 - val_loss: 1.2421 - val_acc: 0.7302\n",
      "Epoch 50/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.8861 - acc: 0.7931 - val_loss: 1.2576 - val_acc: 0.7419\n",
      "Epoch 51/250\n",
      "937/937 [==============================] - 174s 185ms/step - loss: 0.8853 - acc: 0.7938 - val_loss: 1.2237 - val_acc: 0.7444\n",
      "Epoch 52/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8888 - acc: 0.7935 - val_loss: 0.8825 - val_acc: 0.8026\n",
      "Epoch 53/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.8788 - acc: 0.7979 - val_loss: 1.1818 - val_acc: 0.7471\n",
      "Epoch 54/250\n",
      "937/937 [==============================] - 173s 184ms/step - loss: 0.8845 - acc: 0.7960 - val_loss: 1.3480 - val_acc: 0.6926\n",
      "Epoch 55/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.8887 - acc: 0.7959 - val_loss: 1.1757 - val_acc: 0.7182\n",
      "Epoch 56/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8806 - acc: 0.7962 - val_loss: 0.9454 - val_acc: 0.7915\n",
      "Epoch 57/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8791 - acc: 0.7973 - val_loss: 1.0719 - val_acc: 0.7574\n",
      "Epoch 58/250\n",
      "937/937 [==============================] - 173s 185ms/step - loss: 0.8823 - acc: 0.7969 - val_loss: 0.9426 - val_acc: 0.7831\n",
      "Epoch 59/250\n",
      "937/937 [==============================] - 174s 186ms/step - loss: 0.8736 - acc: 0.8014 - val_loss: 0.9193 - val_acc: 0.7939\n",
      "Epoch 60/250\n",
      "936/937 [============================>.] - ETA: 0s - loss: 0.8780 - acc: 0.8001"
     ]
    }
   ],
   "source": [
    "# Augmenting the images\n",
    "data_gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                              rotation_range = 15,\n",
    "                              width_shift_range=0.125,\n",
    "                              height_shift_range=0.125,\n",
    "                              fill_mode='nearest')\n",
    "\n",
    "data_iter = data_gen.flow(x_train, y_train, batch_size=64, shuffle=True)\n",
    "\n",
    "model.fit_generator(data_iter,\n",
    "                    samples_per_epoch=60000,\n",
    "                    steps_per_epoch=len(x_train)/64,\n",
    "                    epochs=250,\n",
    "                    verbose = 1,\n",
    "                    validation_data=(x_test,y_test),\n",
    "                    callbacks=[change_lr, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxm7Ut06ZT8x"
   },
   "source": [
    "In the above training output, the jupyter lab was not responding, so, you don't see the output for all the logs, but, the model was running on the server. Below, I have plotted the validation accuracy and training accuracy to get the overall picture. I have also saved the history in a pandas dataframe to show the model accuracy for all the epochs. Also, I have trained the model for last 50 epochs of the 250 epochs without augmenting the images where you get to see the logs for the last 50 epochs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "d91ac1bd-319b-45cc-9965-4ee317cd23b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 1ms/step\n",
      "Test loss: 0.41322941830158233\n",
      "Test accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yVM6YYpcIgc"
   },
   "source": [
    "So, training the dataset with augmentation crosses the benchmark of 92%. Infact, at epoch number 222, the model attains validation accuracy of 92.39%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "50831316-761a-4ea7-9100-cddba5b0be1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save(\"DNST_Final_Aug_epochs_250.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t76F1cWZT87"
   },
   "source": [
    "Storing model logs in a pandas dataframe and printing the logs below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMT42cwfHdpm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 250\n",
    "log_df = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqH0rJzuZT89",
    "outputId": "bed79337-40d7-45d9-ae83-0c466d29591d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.281158</td>\n",
       "      <td>4.574391</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.3775</td>\n",
       "      <td>3.823143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.375167</td>\n",
       "      <td>3.537856</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.3546</td>\n",
       "      <td>3.271892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.433545</td>\n",
       "      <td>2.781619</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.4714</td>\n",
       "      <td>2.471615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.480658</td>\n",
       "      <td>2.295935</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.5462</td>\n",
       "      <td>1.982199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516689</td>\n",
       "      <td>1.949557</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.5660</td>\n",
       "      <td>1.768489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.549121</td>\n",
       "      <td>1.706527</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>1.561244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.584296</td>\n",
       "      <td>1.512141</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>1.780126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.614853</td>\n",
       "      <td>1.368749</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.5715</td>\n",
       "      <td>1.671411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.648114</td>\n",
       "      <td>1.243418</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6713</td>\n",
       "      <td>1.203994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.666088</td>\n",
       "      <td>1.165405</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>1.334243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.685329</td>\n",
       "      <td>1.107537</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7173</td>\n",
       "      <td>1.071291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.700851</td>\n",
       "      <td>1.062037</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7062</td>\n",
       "      <td>1.175396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.712083</td>\n",
       "      <td>1.031188</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6777</td>\n",
       "      <td>1.278484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.722196</td>\n",
       "      <td>1.006494</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>1.518592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.729339</td>\n",
       "      <td>1.003127</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>1.204580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.735686</td>\n",
       "      <td>0.978575</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6034</td>\n",
       "      <td>1.969756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.737517</td>\n",
       "      <td>0.974517</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>0.967930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.743992</td>\n",
       "      <td>0.962314</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6662</td>\n",
       "      <td>1.299341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.751335</td>\n",
       "      <td>0.951299</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>1.262082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.754756</td>\n",
       "      <td>0.946380</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7604</td>\n",
       "      <td>1.015470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.758234</td>\n",
       "      <td>0.938412</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>1.158569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.757944</td>\n",
       "      <td>0.941129</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7228</td>\n",
       "      <td>1.206674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.762667</td>\n",
       "      <td>0.932527</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7006</td>\n",
       "      <td>1.276513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.765804</td>\n",
       "      <td>0.926112</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>0.874327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.769209</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7485</td>\n",
       "      <td>1.114638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.768840</td>\n",
       "      <td>0.924831</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.984821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.773765</td>\n",
       "      <td>0.910683</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.966896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.773531</td>\n",
       "      <td>0.916806</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7461</td>\n",
       "      <td>1.070072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.776519</td>\n",
       "      <td>0.911195</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>1.094561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.773715</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7274</td>\n",
       "      <td>1.274920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.775471</td>\n",
       "      <td>0.919737</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>1.165139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.779456</td>\n",
       "      <td>0.907840</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>1.103299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.905244</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.944061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.782226</td>\n",
       "      <td>0.901182</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>1.165223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.902144</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>1.612764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.778962</td>\n",
       "      <td>0.911410</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7616</td>\n",
       "      <td>1.099209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.784963</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.884386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.785965</td>\n",
       "      <td>0.900871</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7752</td>\n",
       "      <td>1.015365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.788151</td>\n",
       "      <td>0.894011</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7501</td>\n",
       "      <td>1.187426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.785147</td>\n",
       "      <td>0.902571</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7312</td>\n",
       "      <td>1.159344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.788499</td>\n",
       "      <td>0.893252</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7207</td>\n",
       "      <td>1.275051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.788535</td>\n",
       "      <td>0.897990</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>1.031282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.790771</td>\n",
       "      <td>0.892454</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7467</td>\n",
       "      <td>1.220187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.789119</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.889313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.791021</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>1.030564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.793543</td>\n",
       "      <td>0.889532</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.979258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.791539</td>\n",
       "      <td>0.889434</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7642</td>\n",
       "      <td>1.047925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.791789</td>\n",
       "      <td>0.887653</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7219</td>\n",
       "      <td>1.267549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.794877</td>\n",
       "      <td>0.883595</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7302</td>\n",
       "      <td>1.242102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.793124</td>\n",
       "      <td>0.886069</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7419</td>\n",
       "      <td>1.257599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.793760</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>1.223746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.793558</td>\n",
       "      <td>0.888524</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.882512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.878890</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7471</td>\n",
       "      <td>1.181767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.796228</td>\n",
       "      <td>0.884263</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6926</td>\n",
       "      <td>1.347973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.795945</td>\n",
       "      <td>0.888531</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>1.175672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.796299</td>\n",
       "      <td>0.880470</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.945409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.797296</td>\n",
       "      <td>0.879239</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7574</td>\n",
       "      <td>1.071946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.796913</td>\n",
       "      <td>0.882267</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>0.942593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.801368</td>\n",
       "      <td>0.873740</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7939</td>\n",
       "      <td>0.919292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.800200</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>1.462473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.798771</td>\n",
       "      <td>0.874061</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7440</td>\n",
       "      <td>1.227821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.800434</td>\n",
       "      <td>0.874688</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.988183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.799015</td>\n",
       "      <td>0.877008</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>0.906998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.801302</td>\n",
       "      <td>0.870423</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>1.388462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.800684</td>\n",
       "      <td>0.877583</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7574</td>\n",
       "      <td>1.181912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.802746</td>\n",
       "      <td>0.869008</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7955</td>\n",
       "      <td>0.949471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.801218</td>\n",
       "      <td>0.875713</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>1.019907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.803304</td>\n",
       "      <td>0.868892</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8041</td>\n",
       "      <td>0.910334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.801602</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7786</td>\n",
       "      <td>1.007634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.805691</td>\n",
       "      <td>0.865346</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>1.544469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.803564</td>\n",
       "      <td>0.866611</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>1.024250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.806442</td>\n",
       "      <td>0.859832</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7344</td>\n",
       "      <td>1.242505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.803188</td>\n",
       "      <td>0.869104</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.966068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.805524</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6178</td>\n",
       "      <td>1.853673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.803505</td>\n",
       "      <td>0.867852</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>1.041568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.806003</td>\n",
       "      <td>0.860268</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.963718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.806208</td>\n",
       "      <td>0.857898</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.922285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.804640</td>\n",
       "      <td>0.865436</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.043818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.802770</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>1.123323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.805674</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>1.026951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.806554</td>\n",
       "      <td>0.858804</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>1.343560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.806025</td>\n",
       "      <td>0.859585</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8149</td>\n",
       "      <td>0.876905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.806409</td>\n",
       "      <td>0.858396</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7852</td>\n",
       "      <td>1.038660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.805107</td>\n",
       "      <td>0.863573</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.895681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.808445</td>\n",
       "      <td>0.855561</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7874</td>\n",
       "      <td>0.981879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.805201</td>\n",
       "      <td>0.863064</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>1.194080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.807710</td>\n",
       "      <td>0.855929</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7271</td>\n",
       "      <td>1.133928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.806342</td>\n",
       "      <td>0.857340</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>0.872664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.808027</td>\n",
       "      <td>0.857785</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>1.409407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.806091</td>\n",
       "      <td>0.865420</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.358908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.809527</td>\n",
       "      <td>0.858298</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7617</td>\n",
       "      <td>1.140489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.810531</td>\n",
       "      <td>0.854347</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>1.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.808495</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7631</td>\n",
       "      <td>1.142805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.809997</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7649</td>\n",
       "      <td>1.206423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.808929</td>\n",
       "      <td>0.852210</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.881648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.809093</td>\n",
       "      <td>0.855015</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>1.583443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.810347</td>\n",
       "      <td>0.850783</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.967302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.810597</td>\n",
       "      <td>0.851840</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>1.030047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.814202</td>\n",
       "      <td>0.841273</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7713</td>\n",
       "      <td>1.155358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.810614</td>\n",
       "      <td>0.849188</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>1.029015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.810262</td>\n",
       "      <td>0.851756</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>1.026854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.812517</td>\n",
       "      <td>0.846413</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8357</td>\n",
       "      <td>0.825864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.815003</td>\n",
       "      <td>0.843260</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>1.073456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.810113</td>\n",
       "      <td>0.853025</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7433</td>\n",
       "      <td>1.244798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.813117</td>\n",
       "      <td>0.847161</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>1.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.811732</td>\n",
       "      <td>0.844111</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>1.430658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.812283</td>\n",
       "      <td>0.840641</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>1.223944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7363</td>\n",
       "      <td>1.208852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.810614</td>\n",
       "      <td>0.846170</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>1.454006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.811081</td>\n",
       "      <td>0.845101</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>1.047065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.812032</td>\n",
       "      <td>0.842180</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.907855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.813101</td>\n",
       "      <td>0.842753</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7027</td>\n",
       "      <td>1.507952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.815370</td>\n",
       "      <td>0.843071</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>1.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.813702</td>\n",
       "      <td>0.841153</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7495</td>\n",
       "      <td>1.240695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.811866</td>\n",
       "      <td>0.846218</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7480</td>\n",
       "      <td>1.242441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.812033</td>\n",
       "      <td>0.846413</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>1.160933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.813235</td>\n",
       "      <td>0.841774</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.939213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.816722</td>\n",
       "      <td>0.835592</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7969</td>\n",
       "      <td>0.970547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.815320</td>\n",
       "      <td>0.836566</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.817682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.813067</td>\n",
       "      <td>0.840776</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>0.871966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.813218</td>\n",
       "      <td>0.842877</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7901</td>\n",
       "      <td>1.042655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.815256</td>\n",
       "      <td>0.838986</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7963</td>\n",
       "      <td>0.949191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.814753</td>\n",
       "      <td>0.836679</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8034</td>\n",
       "      <td>0.926067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.815237</td>\n",
       "      <td>0.838148</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.979901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.814803</td>\n",
       "      <td>0.836516</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7531</td>\n",
       "      <td>1.067855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.814720</td>\n",
       "      <td>0.838348</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.7447</td>\n",
       "      <td>1.180281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.864177</td>\n",
       "      <td>0.690799</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.674274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.881859</td>\n",
       "      <td>0.629949</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.628729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.885564</td>\n",
       "      <td>0.606451</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.635213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.887717</td>\n",
       "      <td>0.595411</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8852</td>\n",
       "      <td>0.636735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.890738</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8948</td>\n",
       "      <td>0.594837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.895828</td>\n",
       "      <td>0.557261</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8926</td>\n",
       "      <td>0.613845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.896312</td>\n",
       "      <td>0.546317</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>0.599250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.895494</td>\n",
       "      <td>0.539884</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8954</td>\n",
       "      <td>0.566925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.898932</td>\n",
       "      <td>0.525957</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.533324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.901769</td>\n",
       "      <td>0.510789</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.564788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.901874</td>\n",
       "      <td>0.505591</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.587525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.903188</td>\n",
       "      <td>0.495221</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>0.517448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.903838</td>\n",
       "      <td>0.486583</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8924</td>\n",
       "      <td>0.564046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.904322</td>\n",
       "      <td>0.483997</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>0.543709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.904222</td>\n",
       "      <td>0.473070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>0.520675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.906133</td>\n",
       "      <td>0.468450</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.532971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.903338</td>\n",
       "      <td>0.467556</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.532476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.907744</td>\n",
       "      <td>0.453686</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.545183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.452628</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.510873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.905991</td>\n",
       "      <td>0.447842</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9010</td>\n",
       "      <td>0.507043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.907185</td>\n",
       "      <td>0.442364</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>0.506547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.908077</td>\n",
       "      <td>0.436945</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>0.492479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.908695</td>\n",
       "      <td>0.433048</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.552895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.908128</td>\n",
       "      <td>0.428724</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8952</td>\n",
       "      <td>0.511349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.907777</td>\n",
       "      <td>0.430519</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>0.497634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.908805</td>\n",
       "      <td>0.421326</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>0.552516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.909396</td>\n",
       "      <td>0.418479</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.534076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.905474</td>\n",
       "      <td>0.424919</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>0.494221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.910197</td>\n",
       "      <td>0.413875</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9035</td>\n",
       "      <td>0.480902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.908077</td>\n",
       "      <td>0.414236</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9024</td>\n",
       "      <td>0.466880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.404789</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8821</td>\n",
       "      <td>0.552097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.909162</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.512937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.908895</td>\n",
       "      <td>0.407933</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8877</td>\n",
       "      <td>0.522683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.909963</td>\n",
       "      <td>0.402576</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.481394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.909329</td>\n",
       "      <td>0.403852</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8926</td>\n",
       "      <td>0.502462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.909824</td>\n",
       "      <td>0.399309</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8802</td>\n",
       "      <td>0.533447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.911031</td>\n",
       "      <td>0.395514</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>0.484351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.911916</td>\n",
       "      <td>0.392960</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9008</td>\n",
       "      <td>0.469742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.910731</td>\n",
       "      <td>0.396264</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8867</td>\n",
       "      <td>0.516346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.910080</td>\n",
       "      <td>0.395962</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.636328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.910142</td>\n",
       "      <td>0.393212</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>0.582805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.911399</td>\n",
       "      <td>0.390882</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.478148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.912517</td>\n",
       "      <td>0.391586</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>0.443878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.913168</td>\n",
       "      <td>0.386341</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>0.541324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.389770</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>0.483724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.910609</td>\n",
       "      <td>0.387938</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8867</td>\n",
       "      <td>0.504905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.910047</td>\n",
       "      <td>0.388518</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.631266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.912150</td>\n",
       "      <td>0.384813</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8884</td>\n",
       "      <td>0.477791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.912450</td>\n",
       "      <td>0.384980</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>0.537644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.912366</td>\n",
       "      <td>0.379531</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>0.526910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.913081</td>\n",
       "      <td>0.379505</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.467552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.911883</td>\n",
       "      <td>0.382069</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.493149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.912583</td>\n",
       "      <td>0.382355</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>0.442668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.913468</td>\n",
       "      <td>0.377196</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>0.468156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.912567</td>\n",
       "      <td>0.380442</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8923</td>\n",
       "      <td>0.519240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.914100</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.571466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.914653</td>\n",
       "      <td>0.371651</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.593889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.914252</td>\n",
       "      <td>0.377110</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>0.442833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.912984</td>\n",
       "      <td>0.375147</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9001</td>\n",
       "      <td>0.467165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.912634</td>\n",
       "      <td>0.377772</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.526635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.914651</td>\n",
       "      <td>0.372653</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.545690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.913885</td>\n",
       "      <td>0.374215</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.691880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.915070</td>\n",
       "      <td>0.372565</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.534846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.929339</td>\n",
       "      <td>0.333017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.414215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.937250</td>\n",
       "      <td>0.310269</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9165</td>\n",
       "      <td>0.412581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.939354</td>\n",
       "      <td>0.303018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9152</td>\n",
       "      <td>0.414688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.940320</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.416509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.941856</td>\n",
       "      <td>0.295448</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>0.404975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.941973</td>\n",
       "      <td>0.292452</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.415815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.941656</td>\n",
       "      <td>0.291321</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0.398174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.943012</td>\n",
       "      <td>0.290103</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9196</td>\n",
       "      <td>0.406090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.943909</td>\n",
       "      <td>0.286588</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>0.400816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.944459</td>\n",
       "      <td>0.282397</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.421173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.943341</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9201</td>\n",
       "      <td>0.398093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.944493</td>\n",
       "      <td>0.281354</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9176</td>\n",
       "      <td>0.407436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.945734</td>\n",
       "      <td>0.278726</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9205</td>\n",
       "      <td>0.403611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.945778</td>\n",
       "      <td>0.276474</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>0.408325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.947747</td>\n",
       "      <td>0.273314</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>0.403944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.947814</td>\n",
       "      <td>0.272895</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9185</td>\n",
       "      <td>0.414823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.948632</td>\n",
       "      <td>0.271295</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>0.417233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.948974</td>\n",
       "      <td>0.268289</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.416023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.948047</td>\n",
       "      <td>0.270781</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.415273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.948932</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9195</td>\n",
       "      <td>0.407347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.948665</td>\n",
       "      <td>0.269002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9216</td>\n",
       "      <td>0.403388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.268280</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9175</td>\n",
       "      <td>0.414161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.949843</td>\n",
       "      <td>0.263383</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.423263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.948732</td>\n",
       "      <td>0.264237</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9225</td>\n",
       "      <td>0.398689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.950417</td>\n",
       "      <td>0.264387</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9208</td>\n",
       "      <td>0.407112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.948364</td>\n",
       "      <td>0.267179</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>0.405121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.950751</td>\n",
       "      <td>0.259649</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.399346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.949960</td>\n",
       "      <td>0.260074</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9224</td>\n",
       "      <td>0.403419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.951001</td>\n",
       "      <td>0.260513</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9210</td>\n",
       "      <td>0.405909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.951585</td>\n",
       "      <td>0.259354</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9207</td>\n",
       "      <td>0.407003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.952370</td>\n",
       "      <td>0.254303</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>0.406620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.950918</td>\n",
       "      <td>0.258283</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9219</td>\n",
       "      <td>0.401470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.951263</td>\n",
       "      <td>0.256137</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>0.399623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.952637</td>\n",
       "      <td>0.254026</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9239</td>\n",
       "      <td>0.403833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.952603</td>\n",
       "      <td>0.252797</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.414412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.950901</td>\n",
       "      <td>0.254134</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>0.398880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.951252</td>\n",
       "      <td>0.254535</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9244</td>\n",
       "      <td>0.395924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.953755</td>\n",
       "      <td>0.249920</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9213</td>\n",
       "      <td>0.414951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.952682</td>\n",
       "      <td>0.251287</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>0.414564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.952153</td>\n",
       "      <td>0.251820</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9227</td>\n",
       "      <td>0.401084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.952453</td>\n",
       "      <td>0.249218</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>0.409583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.952103</td>\n",
       "      <td>0.250332</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.401140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.953571</td>\n",
       "      <td>0.248315</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9199</td>\n",
       "      <td>0.410410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.952682</td>\n",
       "      <td>0.247952</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9207</td>\n",
       "      <td>0.409068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.953788</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9194</td>\n",
       "      <td>0.415077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.954322</td>\n",
       "      <td>0.244795</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0.414916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.954089</td>\n",
       "      <td>0.245975</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.410669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.954773</td>\n",
       "      <td>0.243532</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9199</td>\n",
       "      <td>0.416713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.954653</td>\n",
       "      <td>0.241847</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>0.404468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.954256</td>\n",
       "      <td>0.241525</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9212</td>\n",
       "      <td>0.407110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.952770</td>\n",
       "      <td>0.243916</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.419333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.954873</td>\n",
       "      <td>0.241081</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9149</td>\n",
       "      <td>0.423269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.953805</td>\n",
       "      <td>0.241286</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>0.406506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.954637</td>\n",
       "      <td>0.240948</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9223</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.954873</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9220</td>\n",
       "      <td>0.401168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.956041</td>\n",
       "      <td>0.238022</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9213</td>\n",
       "      <td>0.401730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.954456</td>\n",
       "      <td>0.241614</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9196</td>\n",
       "      <td>0.413291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.954423</td>\n",
       "      <td>0.239982</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>0.411676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.956708</td>\n",
       "      <td>0.235275</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.417238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.956091</td>\n",
       "      <td>0.235988</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9207</td>\n",
       "      <td>0.404338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.955073</td>\n",
       "      <td>0.235019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>0.413229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc      loss     lr  val_acc  val_loss\n",
       "0    0.281158  4.574391  0.100   0.3775  3.823143\n",
       "1    0.375167  3.537856  0.100   0.3546  3.271892\n",
       "2    0.433545  2.781619  0.100   0.4714  2.471615\n",
       "3    0.480658  2.295935  0.100   0.5462  1.982199\n",
       "4    0.516689  1.949557  0.100   0.5660  1.768489\n",
       "5    0.549121  1.706527  0.100   0.5771  1.561244\n",
       "6    0.584296  1.512141  0.100   0.5689  1.780126\n",
       "7    0.614853  1.368749  0.100   0.5715  1.671411\n",
       "8    0.648114  1.243418  0.100   0.6713  1.203994\n",
       "9    0.666088  1.165405  0.100   0.6505  1.334243\n",
       "10   0.685329  1.107537  0.100   0.7173  1.071291\n",
       "11   0.700851  1.062037  0.100   0.7062  1.175396\n",
       "12   0.712083  1.031188  0.100   0.6777  1.278484\n",
       "13   0.722196  1.006494  0.100   0.6643  1.518592\n",
       "14   0.729339  1.003127  0.100   0.6948  1.204580\n",
       "15   0.735686  0.978575  0.100   0.6034  1.969756\n",
       "16   0.737517  0.974517  0.100   0.7569  0.967930\n",
       "17   0.743992  0.962314  0.100   0.6662  1.299341\n",
       "18   0.751335  0.951299  0.100   0.6976  1.262082\n",
       "19   0.754756  0.946380  0.100   0.7604  1.015470\n",
       "20   0.758234  0.938412  0.100   0.7143  1.158569\n",
       "21   0.757944  0.941129  0.100   0.7228  1.206674\n",
       "22   0.762667  0.932527  0.100   0.7006  1.276513\n",
       "23   0.765804  0.926112  0.100   0.7884  0.874327\n",
       "24   0.769209  0.922468  0.100   0.7485  1.114638\n",
       "25   0.768840  0.924831  0.100   0.7517  0.984821\n",
       "26   0.773765  0.910683  0.100   0.7695  0.966896\n",
       "27   0.773531  0.916806  0.100   0.7461  1.070072\n",
       "28   0.776519  0.911195  0.100   0.7406  1.094561\n",
       "29   0.773715  0.918567  0.100   0.7274  1.274920\n",
       "30   0.775471  0.919737  0.100   0.7347  1.165139\n",
       "31   0.779456  0.907840  0.100   0.7499  1.103299\n",
       "32   0.781609  0.905244  0.100   0.7837  0.944061\n",
       "33   0.782226  0.901182  0.100   0.7123  1.165223\n",
       "34   0.783077  0.902144  0.100   0.6627  1.612764\n",
       "35   0.778962  0.911410  0.100   0.7616  1.099209\n",
       "36   0.784963  0.901625  0.100   0.7975  0.884386\n",
       "37   0.785965  0.900871  0.100   0.7752  1.015365\n",
       "38   0.788151  0.894011  0.100   0.7501  1.187426\n",
       "39   0.785147  0.902571  0.100   0.7312  1.159344\n",
       "40   0.788499  0.893252  0.100   0.7207  1.275051\n",
       "41   0.788535  0.897990  0.100   0.7745  1.031282\n",
       "42   0.790771  0.892454  0.100   0.7467  1.220187\n",
       "43   0.789119  0.898407  0.100   0.7990  0.889313\n",
       "44   0.791021  0.895100  0.100   0.7750  1.030564\n",
       "45   0.793543  0.889532  0.100   0.7738  0.979258\n",
       "46   0.791539  0.889434  0.100   0.7642  1.047925\n",
       "47   0.791789  0.887653  0.100   0.7219  1.267549\n",
       "48   0.794877  0.883595  0.100   0.7302  1.242102\n",
       "49   0.793124  0.886069  0.100   0.7419  1.257599\n",
       "50   0.793760  0.885434  0.100   0.7444  1.223746\n",
       "51   0.793558  0.888524  0.100   0.8026  0.882512\n",
       "52   0.797814  0.878890  0.100   0.7471  1.181767\n",
       "53   0.796228  0.884263  0.100   0.6926  1.347973\n",
       "54   0.795945  0.888531  0.100   0.7182  1.175672\n",
       "55   0.796299  0.880470  0.100   0.7915  0.945409\n",
       "56   0.797296  0.879239  0.100   0.7574  1.071946\n",
       "57   0.796913  0.882267  0.100   0.7831  0.942593\n",
       "58   0.801368  0.873740  0.100   0.7939  0.919292\n",
       "59   0.800200  0.877647  0.100   0.6928  1.462473\n",
       "60   0.798771  0.874061  0.100   0.7440  1.227821\n",
       "61   0.800434  0.874688  0.100   0.7889  0.988183\n",
       "62   0.799015  0.877008  0.100   0.8077  0.906998\n",
       "63   0.801302  0.870423  0.100   0.7310  1.388462\n",
       "64   0.800684  0.877583  0.100   0.7574  1.181912\n",
       "65   0.802746  0.869008  0.100   0.7955  0.949471\n",
       "66   0.801218  0.875713  0.100   0.7745  1.019907\n",
       "67   0.803304  0.868892  0.100   0.8041  0.910334\n",
       "68   0.801602  0.868704  0.100   0.7786  1.007634\n",
       "69   0.805691  0.865346  0.100   0.6831  1.544469\n",
       "70   0.803564  0.866611  0.100   0.7838  1.024250\n",
       "71   0.806442  0.859832  0.100   0.7344  1.242505\n",
       "72   0.803188  0.869104  0.100   0.7910  0.966068\n",
       "73   0.805524  0.857300  0.100   0.6178  1.853673\n",
       "74   0.803505  0.867852  0.100   0.7711  1.041568\n",
       "75   0.806003  0.860268  0.100   0.7924  0.963718\n",
       "76   0.806208  0.857898  0.100   0.8053  0.922285\n",
       "77   0.804640  0.865436  0.100   0.7657  1.043818\n",
       "78   0.802770  0.870000  0.100   0.7519  1.123323\n",
       "79   0.805674  0.862033  0.100   0.7674  1.026951\n",
       "80   0.806554  0.858804  0.100   0.7150  1.343560\n",
       "81   0.806025  0.859585  0.100   0.8149  0.876905\n",
       "82   0.806409  0.858396  0.100   0.7852  1.038660\n",
       "83   0.805107  0.863573  0.100   0.8003  0.895681\n",
       "84   0.808445  0.855561  0.100   0.7874  0.981879\n",
       "85   0.805201  0.863064  0.100   0.7623  1.194080\n",
       "86   0.807710  0.855929  0.100   0.7271  1.133928\n",
       "87   0.806342  0.857340  0.100   0.8184  0.872664\n",
       "88   0.808027  0.857785  0.100   0.7024  1.409407\n",
       "89   0.806091  0.865420  0.100   0.7178  1.358908\n",
       "90   0.809527  0.858298  0.100   0.7617  1.140489\n",
       "91   0.810531  0.854347  0.100   0.7833  1.015551\n",
       "92   0.808495  0.857574  0.100   0.7631  1.142805\n",
       "93   0.809997  0.856164  0.100   0.7649  1.206423\n",
       "94   0.808929  0.852210  0.100   0.8003  0.881648\n",
       "95   0.809093  0.855015  0.100   0.6668  1.583443\n",
       "96   0.810347  0.850783  0.100   0.7892  0.967302\n",
       "97   0.810597  0.851840  0.100   0.7843  1.030047\n",
       "98   0.814202  0.841273  0.100   0.7713  1.155358\n",
       "99   0.810614  0.849188  0.100   0.7864  1.029015\n",
       "100  0.810262  0.851756  0.100   0.7857  1.026854\n",
       "101  0.812517  0.846413  0.100   0.8357  0.825864\n",
       "102  0.815003  0.843260  0.100   0.7779  1.073456\n",
       "103  0.810113  0.853025  0.100   0.7433  1.244798\n",
       "104  0.813117  0.847161  0.100   0.7520  1.161791\n",
       "105  0.811732  0.844111  0.100   0.6979  1.430658\n",
       "106  0.812283  0.840641  0.100   0.7376  1.223944\n",
       "107  0.811966  0.847477  0.100   0.7363  1.208852\n",
       "108  0.810614  0.846170  0.100   0.6837  1.454006\n",
       "109  0.811081  0.845101  0.100   0.7686  1.047065\n",
       "110  0.812032  0.842180  0.100   0.8083  0.907855\n",
       "111  0.813101  0.842753  0.100   0.7027  1.507952\n",
       "112  0.815370  0.843071  0.100   0.7774  1.011924\n",
       "113  0.813702  0.841153  0.100   0.7495  1.240695\n",
       "114  0.811866  0.846218  0.100   0.7480  1.242441\n",
       "115  0.812033  0.846413  0.100   0.7624  1.160933\n",
       "116  0.813235  0.841774  0.100   0.8015  0.939213\n",
       "117  0.816722  0.835592  0.100   0.7969  0.970547\n",
       "118  0.815320  0.836566  0.100   0.8316  0.817682\n",
       "119  0.813067  0.840776  0.100   0.8131  0.871966\n",
       "120  0.813218  0.842877  0.100   0.7901  1.042655\n",
       "121  0.815256  0.838986  0.100   0.7963  0.949191\n",
       "122  0.814753  0.836679  0.100   0.8034  0.926067\n",
       "123  0.815237  0.838148  0.100   0.8019  0.979901\n",
       "124  0.814803  0.836516  0.100   0.7531  1.067855\n",
       "125  0.814720  0.838348  0.100   0.7447  1.180281\n",
       "126  0.864177  0.690799  0.010   0.8828  0.674274\n",
       "127  0.881859  0.629949  0.010   0.8947  0.628729\n",
       "128  0.885564  0.606451  0.010   0.8879  0.635213\n",
       "129  0.887717  0.595411  0.010   0.8852  0.636735\n",
       "130  0.890738  0.575221  0.010   0.8948  0.594837\n",
       "131  0.895828  0.557261  0.010   0.8926  0.613845\n",
       "132  0.896312  0.546317  0.010   0.8933  0.599250\n",
       "133  0.895494  0.539884  0.010   0.8954  0.566925\n",
       "134  0.898932  0.525957  0.010   0.9016  0.533324\n",
       "135  0.901769  0.510789  0.010   0.8968  0.564788\n",
       "136  0.901874  0.505591  0.010   0.8896  0.587525\n",
       "137  0.903188  0.495221  0.010   0.9047  0.517448\n",
       "138  0.903838  0.486583  0.010   0.8924  0.564046\n",
       "139  0.904322  0.483997  0.010   0.8979  0.543709\n",
       "140  0.904222  0.473070  0.010   0.8995  0.520675\n",
       "141  0.906133  0.468450  0.010   0.8958  0.532971\n",
       "142  0.903338  0.467556  0.010   0.8959  0.532476\n",
       "143  0.907744  0.453686  0.010   0.8876  0.545183\n",
       "144  0.905858  0.452628  0.010   0.8961  0.510873\n",
       "145  0.905991  0.447842  0.010   0.9010  0.507043\n",
       "146  0.907185  0.442364  0.010   0.8957  0.506547\n",
       "147  0.908077  0.436945  0.010   0.8982  0.492479\n",
       "148  0.908695  0.433048  0.010   0.8875  0.552895\n",
       "149  0.908128  0.428724  0.010   0.8952  0.511349\n",
       "150  0.907777  0.430519  0.010   0.8940  0.497634\n",
       "151  0.908805  0.421326  0.010   0.8829  0.552516\n",
       "152  0.909396  0.418479  0.010   0.8892  0.534076\n",
       "153  0.905474  0.424919  0.010   0.8999  0.494221\n",
       "154  0.910197  0.413875  0.010   0.9035  0.480902\n",
       "155  0.908077  0.414236  0.010   0.9024  0.466880\n",
       "156  0.911762  0.404789  0.010   0.8821  0.552097\n",
       "157  0.909162  0.408600  0.010   0.8896  0.512937\n",
       "158  0.908895  0.407933  0.010   0.8877  0.522683\n",
       "159  0.909963  0.402576  0.010   0.8959  0.481394\n",
       "160  0.909329  0.403852  0.010   0.8926  0.502462\n",
       "161  0.909824  0.399309  0.010   0.8802  0.533447\n",
       "162  0.911031  0.395514  0.010   0.8977  0.484351\n",
       "163  0.911916  0.392960  0.010   0.9008  0.469742\n",
       "164  0.910731  0.396264  0.010   0.8867  0.516346\n",
       "165  0.910080  0.395962  0.010   0.8587  0.636328\n",
       "166  0.910142  0.393212  0.010   0.8680  0.582805\n",
       "167  0.911399  0.390882  0.010   0.8996  0.478148\n",
       "168  0.912517  0.391586  0.010   0.9051  0.443878\n",
       "169  0.913168  0.386341  0.010   0.8833  0.541324\n",
       "170  0.911332  0.389770  0.010   0.8955  0.483724\n",
       "171  0.910609  0.387938  0.010   0.8867  0.504905\n",
       "172  0.910047  0.388518  0.010   0.8621  0.631266\n",
       "173  0.912150  0.384813  0.010   0.8884  0.477791\n",
       "174  0.912450  0.384980  0.010   0.8916  0.537644\n",
       "175  0.912366  0.379531  0.010   0.8886  0.526910\n",
       "176  0.913081  0.379505  0.010   0.8968  0.467552\n",
       "177  0.911883  0.382069  0.010   0.8958  0.493149\n",
       "178  0.912583  0.382355  0.010   0.9070  0.442668\n",
       "179  0.913468  0.377196  0.010   0.8994  0.468156\n",
       "180  0.912567  0.380442  0.010   0.8923  0.519240\n",
       "181  0.914100  0.377247  0.010   0.8810  0.571466\n",
       "182  0.914653  0.371651  0.010   0.8667  0.593889\n",
       "183  0.914252  0.377110  0.010   0.9078  0.442833\n",
       "184  0.912984  0.375147  0.010   0.9001  0.467165\n",
       "185  0.912634  0.377772  0.010   0.8828  0.526635\n",
       "186  0.914651  0.372653  0.010   0.8827  0.545690\n",
       "187  0.913885  0.374215  0.010   0.8510  0.691880\n",
       "188  0.915070  0.372565  0.010   0.8922  0.534846\n",
       "189  0.929339  0.333017  0.001   0.9151  0.414215\n",
       "190  0.937250  0.310269  0.001   0.9165  0.412581\n",
       "191  0.939354  0.303018  0.001   0.9152  0.414688\n",
       "192  0.940320  0.299536  0.001   0.9150  0.416509\n",
       "193  0.941856  0.295448  0.001   0.9174  0.404975\n",
       "194  0.941973  0.292452  0.001   0.9154  0.415815\n",
       "195  0.941656  0.291321  0.001   0.9217  0.398174\n",
       "196  0.943012  0.290103  0.001   0.9196  0.406090\n",
       "197  0.943909  0.286588  0.001   0.9190  0.400816\n",
       "198  0.944459  0.282397  0.001   0.9166  0.421173\n",
       "199  0.943341  0.284923  0.001   0.9201  0.398093\n",
       "200  0.944493  0.281354  0.001   0.9176  0.407436\n",
       "201  0.945734  0.278726  0.001   0.9205  0.403611\n",
       "202  0.945778  0.276474  0.001   0.9184  0.408325\n",
       "203  0.947747  0.273314  0.001   0.9191  0.403944\n",
       "204  0.947814  0.272895  0.001   0.9185  0.414823\n",
       "205  0.948632  0.271295  0.001   0.9170  0.417233\n",
       "206  0.948974  0.268289  0.001   0.9167  0.416023\n",
       "207  0.948047  0.270781  0.001   0.9162  0.415273\n",
       "208  0.948932  0.266247  0.001   0.9195  0.407347\n",
       "209  0.948665  0.269002  0.001   0.9216  0.403388\n",
       "210  0.948148  0.268280  0.001   0.9175  0.414161\n",
       "211  0.949843  0.263383  0.001   0.9167  0.423263\n",
       "212  0.948732  0.264237  0.001   0.9225  0.398689\n",
       "213  0.950417  0.264387  0.001   0.9208  0.407112\n",
       "214  0.948364  0.267179  0.001   0.9218  0.405121\n",
       "215  0.950751  0.259649  0.001   0.9222  0.399346\n",
       "216  0.949960  0.260074  0.001   0.9224  0.403419\n",
       "217  0.951001  0.260513  0.001   0.9210  0.405909\n",
       "218  0.951585  0.259354  0.001   0.9207  0.407003\n",
       "219  0.952370  0.254303  0.001   0.9218  0.406620\n",
       "220  0.950918  0.258283  0.001   0.9219  0.401470\n",
       "221  0.951263  0.256137  0.001   0.9229  0.399623\n",
       "222  0.952637  0.254026  0.001   0.9239  0.403833\n",
       "223  0.952603  0.252797  0.001   0.9189  0.414412\n",
       "224  0.950901  0.254134  0.001   0.9236  0.398880\n",
       "225  0.951252  0.254535  0.001   0.9244  0.395924\n",
       "226  0.953755  0.249920  0.001   0.9213  0.414951\n",
       "227  0.952682  0.251287  0.001   0.9191  0.414564\n",
       "228  0.952153  0.251820  0.001   0.9227  0.401084\n",
       "229  0.952453  0.249218  0.001   0.9226  0.409583\n",
       "230  0.952103  0.250332  0.001   0.9234  0.401140\n",
       "231  0.953571  0.248315  0.001   0.9199  0.410410\n",
       "232  0.952682  0.247952  0.001   0.9207  0.409068\n",
       "233  0.953788  0.245283  0.001   0.9194  0.415077\n",
       "234  0.954322  0.244795  0.001   0.9217  0.414916\n",
       "235  0.954089  0.245975  0.001   0.9231  0.410669\n",
       "236  0.954773  0.243532  0.001   0.9199  0.416713\n",
       "237  0.954653  0.241847  0.001   0.9191  0.404468\n",
       "238  0.954256  0.241525  0.001   0.9212  0.407110\n",
       "239  0.952770  0.243916  0.001   0.9151  0.419333\n",
       "240  0.954873  0.241081  0.001   0.9149  0.423269\n",
       "241  0.953805  0.241286  0.001   0.9229  0.406506\n",
       "242  0.954637  0.240948  0.001   0.9223  0.403400\n",
       "243  0.954873  0.240800  0.001   0.9220  0.401168\n",
       "244  0.956041  0.238022  0.001   0.9213  0.401730\n",
       "245  0.954456  0.241614  0.001   0.9196  0.413291\n",
       "246  0.954423  0.239982  0.001   0.9229  0.411676\n",
       "247  0.956708  0.235275  0.001   0.9179  0.417238\n",
       "248  0.956091  0.235988  0.001   0.9207  0.404338\n",
       "249  0.955073  0.235019  0.001   0.9203  0.413229"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prq-t_PcctnB"
   },
   "source": [
    "Plotting the accuracy and loss gives a better picture of the model training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eirFP1VqZT9D"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xX4a4PBZT9F",
    "outputId": "b7e2a682-9bf3-4efb-fd8e-4e002d0b096e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(log_df['acc'])\n",
    "plt.plot(log_df['val_acc'])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(['train', 'validation'], loc='upper_left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Yum-hRSZT9J",
    "outputId": "2185a74e-0c25-41ba-b9db-345ac6edcb6d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(log_df['loss'])\n",
    "plt.plot(log_df['val_loss'])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(['train', 'validation'], loc='upper_left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYiNS2EOZT9M"
   },
   "source": [
    "Now, we will try to learn the weights without augmenting the images when we decrease the learning from 0.01 to 0.001. Since, we were saving the best model weights after every 10 epochs, we have weights available for the 200th epoch. So, we will load those weights and train the model from 200th epoch onwards without augmenting the images for 50 more epochs and see the result. So, technically, we will be training for 250 epochs only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcDm_4TKZT9N"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"DNST_Final_200-0.40.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LYrBtbFZT9P"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.001, decay = 1e-6, momentum=0.9, nesterov=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5t-Qh3W3ZT9R"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Heh9ySC6ZT9T"
   },
   "outputs": [],
   "source": [
    "checkpointer_50 = callbacks.ModelCheckpoint(filepath= 'DNST_Final_No_Aug_Last50_{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                                         monitor = 'val_acc',\n",
    "                                         mode = 'max',\n",
    "                                         save_best_only = True,\n",
    "                                         save_weights_only=True,\n",
    "                                         period =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGprm_5oZT9W",
    "outputId": "a72fea22-0d93-49ec-bb18-7d7376e5c766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 173s 3ms/step - loss: 0.2291 - acc: 0.9633 - val_loss: 0.3450 - val_acc: 0.9320\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.2228 - acc: 0.9667 - val_loss: 0.3511 - val_acc: 0.9310\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2172 - acc: 0.9681 - val_loss: 0.3467 - val_acc: 0.9323\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2157 - acc: 0.9682 - val_loss: 0.3471 - val_acc: 0.9330\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2114 - acc: 0.9697 - val_loss: 0.3512 - val_acc: 0.9326\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2119 - acc: 0.9698 - val_loss: 0.3507 - val_acc: 0.9309\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.2089 - acc: 0.9704 - val_loss: 0.3528 - val_acc: 0.9318\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2059 - acc: 0.9710 - val_loss: 0.3588 - val_acc: 0.9308\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2039 - acc: 0.9715 - val_loss: 0.3568 - val_acc: 0.9320\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.2019 - acc: 0.9723 - val_loss: 0.3574 - val_acc: 0.9357\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1995 - acc: 0.9730 - val_loss: 0.3601 - val_acc: 0.9310\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1985 - acc: 0.9737 - val_loss: 0.3628 - val_acc: 0.9327\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.1968 - acc: 0.9738 - val_loss: 0.3575 - val_acc: 0.9316\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1945 - acc: 0.9743 - val_loss: 0.3576 - val_acc: 0.9321\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1925 - acc: 0.9758 - val_loss: 0.3586 - val_acc: 0.9319\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1894 - acc: 0.9763 - val_loss: 0.3637 - val_acc: 0.9302\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1919 - acc: 0.9744 - val_loss: 0.3583 - val_acc: 0.9329\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 146s 3ms/step - loss: 0.1891 - acc: 0.9754 - val_loss: 0.3583 - val_acc: 0.9339\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1863 - acc: 0.9765 - val_loss: 0.3659 - val_acc: 0.9307\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1868 - acc: 0.9764 - val_loss: 0.3626 - val_acc: 0.9325\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1855 - acc: 0.9770 - val_loss: 0.3660 - val_acc: 0.9325\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1848 - acc: 0.9774 - val_loss: 0.3654 - val_acc: 0.9322\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 146s 3ms/step - loss: 0.1833 - acc: 0.9768 - val_loss: 0.3682 - val_acc: 0.9331\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1812 - acc: 0.9784 - val_loss: 0.3791 - val_acc: 0.9297\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 146s 3ms/step - loss: 0.1784 - acc: 0.9786 - val_loss: 0.3766 - val_acc: 0.9311\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1806 - acc: 0.9775 - val_loss: 0.3763 - val_acc: 0.9321\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1761 - acc: 0.9798 - val_loss: 0.3796 - val_acc: 0.9307\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1789 - acc: 0.9784 - val_loss: 0.3723 - val_acc: 0.9325\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1731 - acc: 0.9802 - val_loss: 0.3726 - val_acc: 0.9329\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1770 - acc: 0.9789 - val_loss: 0.3731 - val_acc: 0.9304\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1726 - acc: 0.9799 - val_loss: 0.3741 - val_acc: 0.9307\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.1740 - acc: 0.9795 - val_loss: 0.3743 - val_acc: 0.9315\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1706 - acc: 0.9815 - val_loss: 0.3740 - val_acc: 0.9333\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1692 - acc: 0.9816 - val_loss: 0.3814 - val_acc: 0.9315\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1676 - acc: 0.9816 - val_loss: 0.3866 - val_acc: 0.9289\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1672 - acc: 0.9813 - val_loss: 0.3866 - val_acc: 0.9298\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1670 - acc: 0.9819 - val_loss: 0.3723 - val_acc: 0.9338\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1649 - acc: 0.9824 - val_loss: 0.3801 - val_acc: 0.9301\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1672 - acc: 0.9812 - val_loss: 0.3763 - val_acc: 0.9300\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1639 - acc: 0.9824 - val_loss: 0.3855 - val_acc: 0.9304\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1642 - acc: 0.9817 - val_loss: 0.3822 - val_acc: 0.9319\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1629 - acc: 0.9823 - val_loss: 0.3920 - val_acc: 0.9285\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.1638 - acc: 0.9827 - val_loss: 0.3797 - val_acc: 0.9320\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1597 - acc: 0.9836 - val_loss: 0.3832 - val_acc: 0.9305\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1603 - acc: 0.9830 - val_loss: 0.3886 - val_acc: 0.9301\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1578 - acc: 0.9845 - val_loss: 0.3879 - val_acc: 0.9296\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1575 - acc: 0.9838 - val_loss: 0.3769 - val_acc: 0.9321\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1579 - acc: 0.9836 - val_loss: 0.3764 - val_acc: 0.9330\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1580 - acc: 0.9837 - val_loss: 0.3806 - val_acc: 0.9326\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 0.1580 - acc: 0.9834 - val_loss: 0.3845 - val_acc: 0.9323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd842e40dd8>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs =50,\n",
    "          batch_size = 64,\n",
    "          verbose = 1,\n",
    "          validation_data = [x_test, y_test],\n",
    "          callbacks=[checkpointer_50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_4atJHBZT9a"
   },
   "source": [
    "In the above logs, we see that not augmenting for the last 50 epochs helped in achieving higher accuracy with 93.57 as the highest accuracy at 210th (or 10th epoch in the above output) epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvzkTCgLZT9b",
    "outputId": "d41f26d5-06da-4f5b-d6d4-d1c37b4777a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 770us/step\n",
      "test loss: 0.38449104194641115\n",
      "test accuracy: 0.9323\n"
     ]
    }
   ],
   "source": [
    "better_score = model.evaluate(x_test, y_test, verbose = 1, batch_size=64)\n",
    "print(\"test loss:\", better_score[0])\n",
    "print(\"test accuracy:\", better_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNoWDl7cZT9d",
    "outputId": "478f450d-e731-4711-c41f-8168430548b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save(\"DNST_Final_No_Aug_Last50.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mACNHR1cZT9m",
    "outputId": "54c8af47-00d6-4ae6-93d3-1068b7bd2c2c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96330</td>\n",
       "      <td>0.229079</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>0.345002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96668</td>\n",
       "      <td>0.222764</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.351142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.96810</td>\n",
       "      <td>0.217228</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>0.346746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.96816</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.347083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.96974</td>\n",
       "      <td>0.211427</td>\n",
       "      <td>0.9326</td>\n",
       "      <td>0.351246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc      loss  val_acc  val_loss\n",
       "0  0.96330  0.229079   0.9320  0.345002\n",
       "1  0.96668  0.222764   0.9310  0.351142\n",
       "2  0.96810  0.217228   0.9323  0.346746\n",
       "3  0.96816  0.215706   0.9330  0.347083\n",
       "4  0.96974  0.211427   0.9326  0.351246"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the logs to pandas dataframe\n",
    "better_log_df = pd.DataFrame(model.history.history)\n",
    "better_log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3S7HbByZT9t"
   },
   "outputs": [],
   "source": [
    "# concatenating the output from first 200 epochs with last 50 epochs done above\n",
    "better_log_df =  pd.concat([log_df.iloc[0:200,[0,1,3,4]], better_log_df], axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-znVWMHbZT9w",
    "outputId": "f1a23af8-dded-47e8-99d8-5116b4904ded"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.281158</td>\n",
       "      <td>4.574391</td>\n",
       "      <td>0.3775</td>\n",
       "      <td>3.823143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.375167</td>\n",
       "      <td>3.537856</td>\n",
       "      <td>0.3546</td>\n",
       "      <td>3.271892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.433545</td>\n",
       "      <td>2.781619</td>\n",
       "      <td>0.4714</td>\n",
       "      <td>2.471615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.480658</td>\n",
       "      <td>2.295935</td>\n",
       "      <td>0.5462</td>\n",
       "      <td>1.982199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516689</td>\n",
       "      <td>1.949557</td>\n",
       "      <td>0.5660</td>\n",
       "      <td>1.768489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.549121</td>\n",
       "      <td>1.706527</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>1.561244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.584296</td>\n",
       "      <td>1.512141</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>1.780126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.614853</td>\n",
       "      <td>1.368749</td>\n",
       "      <td>0.5715</td>\n",
       "      <td>1.671411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.648114</td>\n",
       "      <td>1.243418</td>\n",
       "      <td>0.6713</td>\n",
       "      <td>1.203994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.666088</td>\n",
       "      <td>1.165405</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>1.334243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.685329</td>\n",
       "      <td>1.107537</td>\n",
       "      <td>0.7173</td>\n",
       "      <td>1.071291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.700851</td>\n",
       "      <td>1.062037</td>\n",
       "      <td>0.7062</td>\n",
       "      <td>1.175396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.712083</td>\n",
       "      <td>1.031188</td>\n",
       "      <td>0.6777</td>\n",
       "      <td>1.278484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.722196</td>\n",
       "      <td>1.006494</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>1.518592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.729339</td>\n",
       "      <td>1.003127</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>1.204580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.735686</td>\n",
       "      <td>0.978575</td>\n",
       "      <td>0.6034</td>\n",
       "      <td>1.969756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.737517</td>\n",
       "      <td>0.974517</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>0.967930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.743992</td>\n",
       "      <td>0.962314</td>\n",
       "      <td>0.6662</td>\n",
       "      <td>1.299341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.751335</td>\n",
       "      <td>0.951299</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>1.262082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.754756</td>\n",
       "      <td>0.946380</td>\n",
       "      <td>0.7604</td>\n",
       "      <td>1.015470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.758234</td>\n",
       "      <td>0.938412</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>1.158569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.757944</td>\n",
       "      <td>0.941129</td>\n",
       "      <td>0.7228</td>\n",
       "      <td>1.206674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.762667</td>\n",
       "      <td>0.932527</td>\n",
       "      <td>0.7006</td>\n",
       "      <td>1.276513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.765804</td>\n",
       "      <td>0.926112</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>0.874327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.769209</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>0.7485</td>\n",
       "      <td>1.114638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.768840</td>\n",
       "      <td>0.924831</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.984821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.773765</td>\n",
       "      <td>0.910683</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.966896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.773531</td>\n",
       "      <td>0.916806</td>\n",
       "      <td>0.7461</td>\n",
       "      <td>1.070072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.776519</td>\n",
       "      <td>0.911195</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>1.094561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.773715</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>0.7274</td>\n",
       "      <td>1.274920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.775471</td>\n",
       "      <td>0.919737</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>1.165139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.779456</td>\n",
       "      <td>0.907840</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>1.103299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.905244</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.944061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.782226</td>\n",
       "      <td>0.901182</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>1.165223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.902144</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>1.612764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.778962</td>\n",
       "      <td>0.911410</td>\n",
       "      <td>0.7616</td>\n",
       "      <td>1.099209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.784963</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.884386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.785965</td>\n",
       "      <td>0.900871</td>\n",
       "      <td>0.7752</td>\n",
       "      <td>1.015365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.788151</td>\n",
       "      <td>0.894011</td>\n",
       "      <td>0.7501</td>\n",
       "      <td>1.187426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.785147</td>\n",
       "      <td>0.902571</td>\n",
       "      <td>0.7312</td>\n",
       "      <td>1.159344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.788499</td>\n",
       "      <td>0.893252</td>\n",
       "      <td>0.7207</td>\n",
       "      <td>1.275051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.788535</td>\n",
       "      <td>0.897990</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>1.031282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.790771</td>\n",
       "      <td>0.892454</td>\n",
       "      <td>0.7467</td>\n",
       "      <td>1.220187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.789119</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.889313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.791021</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>1.030564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.793543</td>\n",
       "      <td>0.889532</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.979258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.791539</td>\n",
       "      <td>0.889434</td>\n",
       "      <td>0.7642</td>\n",
       "      <td>1.047925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.791789</td>\n",
       "      <td>0.887653</td>\n",
       "      <td>0.7219</td>\n",
       "      <td>1.267549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.794877</td>\n",
       "      <td>0.883595</td>\n",
       "      <td>0.7302</td>\n",
       "      <td>1.242102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.793124</td>\n",
       "      <td>0.886069</td>\n",
       "      <td>0.7419</td>\n",
       "      <td>1.257599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.793760</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>1.223746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.793558</td>\n",
       "      <td>0.888524</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.882512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.878890</td>\n",
       "      <td>0.7471</td>\n",
       "      <td>1.181767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.796228</td>\n",
       "      <td>0.884263</td>\n",
       "      <td>0.6926</td>\n",
       "      <td>1.347973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.795945</td>\n",
       "      <td>0.888531</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>1.175672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.796299</td>\n",
       "      <td>0.880470</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.945409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.797296</td>\n",
       "      <td>0.879239</td>\n",
       "      <td>0.7574</td>\n",
       "      <td>1.071946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.796913</td>\n",
       "      <td>0.882267</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>0.942593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.801368</td>\n",
       "      <td>0.873740</td>\n",
       "      <td>0.7939</td>\n",
       "      <td>0.919292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.800200</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>1.462473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.798771</td>\n",
       "      <td>0.874061</td>\n",
       "      <td>0.7440</td>\n",
       "      <td>1.227821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.800434</td>\n",
       "      <td>0.874688</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.988183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.799015</td>\n",
       "      <td>0.877008</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>0.906998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.801302</td>\n",
       "      <td>0.870423</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>1.388462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.800684</td>\n",
       "      <td>0.877583</td>\n",
       "      <td>0.7574</td>\n",
       "      <td>1.181912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.802746</td>\n",
       "      <td>0.869008</td>\n",
       "      <td>0.7955</td>\n",
       "      <td>0.949471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.801218</td>\n",
       "      <td>0.875713</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>1.019907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.803304</td>\n",
       "      <td>0.868892</td>\n",
       "      <td>0.8041</td>\n",
       "      <td>0.910334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.801602</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.7786</td>\n",
       "      <td>1.007634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.805691</td>\n",
       "      <td>0.865346</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>1.544469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.803564</td>\n",
       "      <td>0.866611</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>1.024250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.806442</td>\n",
       "      <td>0.859832</td>\n",
       "      <td>0.7344</td>\n",
       "      <td>1.242505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.803188</td>\n",
       "      <td>0.869104</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.966068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.805524</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.6178</td>\n",
       "      <td>1.853673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.803505</td>\n",
       "      <td>0.867852</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>1.041568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.806003</td>\n",
       "      <td>0.860268</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.963718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.806208</td>\n",
       "      <td>0.857898</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.922285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.804640</td>\n",
       "      <td>0.865436</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.043818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.802770</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>1.123323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.805674</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>1.026951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.806554</td>\n",
       "      <td>0.858804</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>1.343560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.806025</td>\n",
       "      <td>0.859585</td>\n",
       "      <td>0.8149</td>\n",
       "      <td>0.876905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.806409</td>\n",
       "      <td>0.858396</td>\n",
       "      <td>0.7852</td>\n",
       "      <td>1.038660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.805107</td>\n",
       "      <td>0.863573</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.895681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.808445</td>\n",
       "      <td>0.855561</td>\n",
       "      <td>0.7874</td>\n",
       "      <td>0.981879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.805201</td>\n",
       "      <td>0.863064</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>1.194080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.807710</td>\n",
       "      <td>0.855929</td>\n",
       "      <td>0.7271</td>\n",
       "      <td>1.133928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.806342</td>\n",
       "      <td>0.857340</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>0.872664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.808027</td>\n",
       "      <td>0.857785</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>1.409407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.806091</td>\n",
       "      <td>0.865420</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.358908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.809527</td>\n",
       "      <td>0.858298</td>\n",
       "      <td>0.7617</td>\n",
       "      <td>1.140489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.810531</td>\n",
       "      <td>0.854347</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>1.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.808495</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.7631</td>\n",
       "      <td>1.142805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.809997</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>0.7649</td>\n",
       "      <td>1.206423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.808929</td>\n",
       "      <td>0.852210</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.881648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.809093</td>\n",
       "      <td>0.855015</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>1.583443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.810347</td>\n",
       "      <td>0.850783</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.967302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.810597</td>\n",
       "      <td>0.851840</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>1.030047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.814202</td>\n",
       "      <td>0.841273</td>\n",
       "      <td>0.7713</td>\n",
       "      <td>1.155358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.810614</td>\n",
       "      <td>0.849188</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>1.029015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.810262</td>\n",
       "      <td>0.851756</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>1.026854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.812517</td>\n",
       "      <td>0.846413</td>\n",
       "      <td>0.8357</td>\n",
       "      <td>0.825864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.815003</td>\n",
       "      <td>0.843260</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>1.073456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.810113</td>\n",
       "      <td>0.853025</td>\n",
       "      <td>0.7433</td>\n",
       "      <td>1.244798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.813117</td>\n",
       "      <td>0.847161</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>1.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.811732</td>\n",
       "      <td>0.844111</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>1.430658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.812283</td>\n",
       "      <td>0.840641</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>1.223944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.7363</td>\n",
       "      <td>1.208852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.810614</td>\n",
       "      <td>0.846170</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>1.454006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.811081</td>\n",
       "      <td>0.845101</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>1.047065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.812032</td>\n",
       "      <td>0.842180</td>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.907855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.813101</td>\n",
       "      <td>0.842753</td>\n",
       "      <td>0.7027</td>\n",
       "      <td>1.507952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.815370</td>\n",
       "      <td>0.843071</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>1.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.813702</td>\n",
       "      <td>0.841153</td>\n",
       "      <td>0.7495</td>\n",
       "      <td>1.240695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.811866</td>\n",
       "      <td>0.846218</td>\n",
       "      <td>0.7480</td>\n",
       "      <td>1.242441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.812033</td>\n",
       "      <td>0.846413</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>1.160933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.813235</td>\n",
       "      <td>0.841774</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.939213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.816722</td>\n",
       "      <td>0.835592</td>\n",
       "      <td>0.7969</td>\n",
       "      <td>0.970547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.815320</td>\n",
       "      <td>0.836566</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.817682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.813067</td>\n",
       "      <td>0.840776</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>0.871966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.813218</td>\n",
       "      <td>0.842877</td>\n",
       "      <td>0.7901</td>\n",
       "      <td>1.042655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.815256</td>\n",
       "      <td>0.838986</td>\n",
       "      <td>0.7963</td>\n",
       "      <td>0.949191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.814753</td>\n",
       "      <td>0.836679</td>\n",
       "      <td>0.8034</td>\n",
       "      <td>0.926067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.815237</td>\n",
       "      <td>0.838148</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.979901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.814803</td>\n",
       "      <td>0.836516</td>\n",
       "      <td>0.7531</td>\n",
       "      <td>1.067855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.814720</td>\n",
       "      <td>0.838348</td>\n",
       "      <td>0.7447</td>\n",
       "      <td>1.180281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.864177</td>\n",
       "      <td>0.690799</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.674274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.881859</td>\n",
       "      <td>0.629949</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.628729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.885564</td>\n",
       "      <td>0.606451</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.635213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.887717</td>\n",
       "      <td>0.595411</td>\n",
       "      <td>0.8852</td>\n",
       "      <td>0.636735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.890738</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.8948</td>\n",
       "      <td>0.594837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.895828</td>\n",
       "      <td>0.557261</td>\n",
       "      <td>0.8926</td>\n",
       "      <td>0.613845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.896312</td>\n",
       "      <td>0.546317</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>0.599250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.895494</td>\n",
       "      <td>0.539884</td>\n",
       "      <td>0.8954</td>\n",
       "      <td>0.566925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.898932</td>\n",
       "      <td>0.525957</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.533324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.901769</td>\n",
       "      <td>0.510789</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.564788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.901874</td>\n",
       "      <td>0.505591</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.587525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.903188</td>\n",
       "      <td>0.495221</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>0.517448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.903838</td>\n",
       "      <td>0.486583</td>\n",
       "      <td>0.8924</td>\n",
       "      <td>0.564046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.904322</td>\n",
       "      <td>0.483997</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>0.543709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.904222</td>\n",
       "      <td>0.473070</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>0.520675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.906133</td>\n",
       "      <td>0.468450</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.532971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.903338</td>\n",
       "      <td>0.467556</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.532476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.907744</td>\n",
       "      <td>0.453686</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.545183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.452628</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.510873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.905991</td>\n",
       "      <td>0.447842</td>\n",
       "      <td>0.9010</td>\n",
       "      <td>0.507043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.907185</td>\n",
       "      <td>0.442364</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>0.506547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.908077</td>\n",
       "      <td>0.436945</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>0.492479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.908695</td>\n",
       "      <td>0.433048</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.552895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.908128</td>\n",
       "      <td>0.428724</td>\n",
       "      <td>0.8952</td>\n",
       "      <td>0.511349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.907777</td>\n",
       "      <td>0.430519</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>0.497634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.908805</td>\n",
       "      <td>0.421326</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>0.552516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.909396</td>\n",
       "      <td>0.418479</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.534076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.905474</td>\n",
       "      <td>0.424919</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>0.494221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.910197</td>\n",
       "      <td>0.413875</td>\n",
       "      <td>0.9035</td>\n",
       "      <td>0.480902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.908077</td>\n",
       "      <td>0.414236</td>\n",
       "      <td>0.9024</td>\n",
       "      <td>0.466880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.404789</td>\n",
       "      <td>0.8821</td>\n",
       "      <td>0.552097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.909162</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.512937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.908895</td>\n",
       "      <td>0.407933</td>\n",
       "      <td>0.8877</td>\n",
       "      <td>0.522683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.909963</td>\n",
       "      <td>0.402576</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.481394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.909329</td>\n",
       "      <td>0.403852</td>\n",
       "      <td>0.8926</td>\n",
       "      <td>0.502462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.909824</td>\n",
       "      <td>0.399309</td>\n",
       "      <td>0.8802</td>\n",
       "      <td>0.533447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.911031</td>\n",
       "      <td>0.395514</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>0.484351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.911916</td>\n",
       "      <td>0.392960</td>\n",
       "      <td>0.9008</td>\n",
       "      <td>0.469742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.910731</td>\n",
       "      <td>0.396264</td>\n",
       "      <td>0.8867</td>\n",
       "      <td>0.516346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.910080</td>\n",
       "      <td>0.395962</td>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.636328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.910142</td>\n",
       "      <td>0.393212</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>0.582805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.911399</td>\n",
       "      <td>0.390882</td>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.478148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.912517</td>\n",
       "      <td>0.391586</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>0.443878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.913168</td>\n",
       "      <td>0.386341</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>0.541324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.389770</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>0.483724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.910609</td>\n",
       "      <td>0.387938</td>\n",
       "      <td>0.8867</td>\n",
       "      <td>0.504905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.910047</td>\n",
       "      <td>0.388518</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.631266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.912150</td>\n",
       "      <td>0.384813</td>\n",
       "      <td>0.8884</td>\n",
       "      <td>0.477791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.912450</td>\n",
       "      <td>0.384980</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>0.537644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.912366</td>\n",
       "      <td>0.379531</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>0.526910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.913081</td>\n",
       "      <td>0.379505</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.467552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.911883</td>\n",
       "      <td>0.382069</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.493149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.912583</td>\n",
       "      <td>0.382355</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>0.442668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.913468</td>\n",
       "      <td>0.377196</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>0.468156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.912567</td>\n",
       "      <td>0.380442</td>\n",
       "      <td>0.8923</td>\n",
       "      <td>0.519240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.914100</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.571466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.914653</td>\n",
       "      <td>0.371651</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.593889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.914252</td>\n",
       "      <td>0.377110</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>0.442833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.912984</td>\n",
       "      <td>0.375147</td>\n",
       "      <td>0.9001</td>\n",
       "      <td>0.467165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.912634</td>\n",
       "      <td>0.377772</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.526635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.914651</td>\n",
       "      <td>0.372653</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.545690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.913885</td>\n",
       "      <td>0.374215</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.691880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.915070</td>\n",
       "      <td>0.372565</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.534846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.929339</td>\n",
       "      <td>0.333017</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.414215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.937250</td>\n",
       "      <td>0.310269</td>\n",
       "      <td>0.9165</td>\n",
       "      <td>0.412581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.939354</td>\n",
       "      <td>0.303018</td>\n",
       "      <td>0.9152</td>\n",
       "      <td>0.414688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.940320</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.416509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.941856</td>\n",
       "      <td>0.295448</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>0.404975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.941973</td>\n",
       "      <td>0.292452</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.415815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.941656</td>\n",
       "      <td>0.291321</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0.398174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.943012</td>\n",
       "      <td>0.290103</td>\n",
       "      <td>0.9196</td>\n",
       "      <td>0.406090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.943909</td>\n",
       "      <td>0.286588</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>0.400816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.944459</td>\n",
       "      <td>0.282397</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.421173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.943341</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>0.9201</td>\n",
       "      <td>0.398093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.963300</td>\n",
       "      <td>0.229079</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>0.345002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.966680</td>\n",
       "      <td>0.222764</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.351142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.968100</td>\n",
       "      <td>0.217228</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>0.346746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.968160</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.347083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.969740</td>\n",
       "      <td>0.211427</td>\n",
       "      <td>0.9326</td>\n",
       "      <td>0.351246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.969820</td>\n",
       "      <td>0.211918</td>\n",
       "      <td>0.9309</td>\n",
       "      <td>0.350650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.970400</td>\n",
       "      <td>0.208864</td>\n",
       "      <td>0.9318</td>\n",
       "      <td>0.352849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.971040</td>\n",
       "      <td>0.205855</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.358757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.971520</td>\n",
       "      <td>0.203902</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>0.356844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.972300</td>\n",
       "      <td>0.201872</td>\n",
       "      <td>0.9357</td>\n",
       "      <td>0.357375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.973040</td>\n",
       "      <td>0.199459</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.360102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.973740</td>\n",
       "      <td>0.198549</td>\n",
       "      <td>0.9327</td>\n",
       "      <td>0.362790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.973760</td>\n",
       "      <td>0.196791</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.357505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.974320</td>\n",
       "      <td>0.194484</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>0.357628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.975820</td>\n",
       "      <td>0.192466</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>0.358587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.976320</td>\n",
       "      <td>0.189446</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>0.363701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.974380</td>\n",
       "      <td>0.191858</td>\n",
       "      <td>0.9329</td>\n",
       "      <td>0.358301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.975440</td>\n",
       "      <td>0.189139</td>\n",
       "      <td>0.9339</td>\n",
       "      <td>0.358287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.976500</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>0.365875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.976420</td>\n",
       "      <td>0.186819</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.362609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.976960</td>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.365964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.977420</td>\n",
       "      <td>0.184798</td>\n",
       "      <td>0.9322</td>\n",
       "      <td>0.365393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.976780</td>\n",
       "      <td>0.183331</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.368212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.181206</td>\n",
       "      <td>0.9297</td>\n",
       "      <td>0.379115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.978560</td>\n",
       "      <td>0.178436</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>0.376638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.977480</td>\n",
       "      <td>0.180566</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>0.376282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.979800</td>\n",
       "      <td>0.176131</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>0.379610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.178882</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.372252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.980240</td>\n",
       "      <td>0.173107</td>\n",
       "      <td>0.9329</td>\n",
       "      <td>0.372573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.978860</td>\n",
       "      <td>0.176961</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>0.373105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.979860</td>\n",
       "      <td>0.172621</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>0.374098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.979480</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.374350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.981520</td>\n",
       "      <td>0.170646</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.373972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.981580</td>\n",
       "      <td>0.169244</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.381402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.981640</td>\n",
       "      <td>0.167597</td>\n",
       "      <td>0.9289</td>\n",
       "      <td>0.386560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.981340</td>\n",
       "      <td>0.167219</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.386616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.981940</td>\n",
       "      <td>0.167041</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>0.372294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.982400</td>\n",
       "      <td>0.164932</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.380137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.981160</td>\n",
       "      <td>0.167158</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.376325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.982420</td>\n",
       "      <td>0.163876</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>0.385472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.981720</td>\n",
       "      <td>0.164240</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>0.382230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.982300</td>\n",
       "      <td>0.162944</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>0.391983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.982700</td>\n",
       "      <td>0.163842</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>0.379650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.983640</td>\n",
       "      <td>0.159734</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>0.383228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.983000</td>\n",
       "      <td>0.160271</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.388591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.984500</td>\n",
       "      <td>0.157771</td>\n",
       "      <td>0.9296</td>\n",
       "      <td>0.387852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.983780</td>\n",
       "      <td>0.157502</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>0.376884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.983600</td>\n",
       "      <td>0.157877</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.376353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.157962</td>\n",
       "      <td>0.9326</td>\n",
       "      <td>0.380552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.983400</td>\n",
       "      <td>0.158030</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>0.384491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc      loss  val_acc  val_loss\n",
       "0    0.281158  4.574391   0.3775  3.823143\n",
       "1    0.375167  3.537856   0.3546  3.271892\n",
       "2    0.433545  2.781619   0.4714  2.471615\n",
       "3    0.480658  2.295935   0.5462  1.982199\n",
       "4    0.516689  1.949557   0.5660  1.768489\n",
       "5    0.549121  1.706527   0.5771  1.561244\n",
       "6    0.584296  1.512141   0.5689  1.780126\n",
       "7    0.614853  1.368749   0.5715  1.671411\n",
       "8    0.648114  1.243418   0.6713  1.203994\n",
       "9    0.666088  1.165405   0.6505  1.334243\n",
       "10   0.685329  1.107537   0.7173  1.071291\n",
       "11   0.700851  1.062037   0.7062  1.175396\n",
       "12   0.712083  1.031188   0.6777  1.278484\n",
       "13   0.722196  1.006494   0.6643  1.518592\n",
       "14   0.729339  1.003127   0.6948  1.204580\n",
       "15   0.735686  0.978575   0.6034  1.969756\n",
       "16   0.737517  0.974517   0.7569  0.967930\n",
       "17   0.743992  0.962314   0.6662  1.299341\n",
       "18   0.751335  0.951299   0.6976  1.262082\n",
       "19   0.754756  0.946380   0.7604  1.015470\n",
       "20   0.758234  0.938412   0.7143  1.158569\n",
       "21   0.757944  0.941129   0.7228  1.206674\n",
       "22   0.762667  0.932527   0.7006  1.276513\n",
       "23   0.765804  0.926112   0.7884  0.874327\n",
       "24   0.769209  0.922468   0.7485  1.114638\n",
       "25   0.768840  0.924831   0.7517  0.984821\n",
       "26   0.773765  0.910683   0.7695  0.966896\n",
       "27   0.773531  0.916806   0.7461  1.070072\n",
       "28   0.776519  0.911195   0.7406  1.094561\n",
       "29   0.773715  0.918567   0.7274  1.274920\n",
       "30   0.775471  0.919737   0.7347  1.165139\n",
       "31   0.779456  0.907840   0.7499  1.103299\n",
       "32   0.781609  0.905244   0.7837  0.944061\n",
       "33   0.782226  0.901182   0.7123  1.165223\n",
       "34   0.783077  0.902144   0.6627  1.612764\n",
       "35   0.778962  0.911410   0.7616  1.099209\n",
       "36   0.784963  0.901625   0.7975  0.884386\n",
       "37   0.785965  0.900871   0.7752  1.015365\n",
       "38   0.788151  0.894011   0.7501  1.187426\n",
       "39   0.785147  0.902571   0.7312  1.159344\n",
       "40   0.788499  0.893252   0.7207  1.275051\n",
       "41   0.788535  0.897990   0.7745  1.031282\n",
       "42   0.790771  0.892454   0.7467  1.220187\n",
       "43   0.789119  0.898407   0.7990  0.889313\n",
       "44   0.791021  0.895100   0.7750  1.030564\n",
       "45   0.793543  0.889532   0.7738  0.979258\n",
       "46   0.791539  0.889434   0.7642  1.047925\n",
       "47   0.791789  0.887653   0.7219  1.267549\n",
       "48   0.794877  0.883595   0.7302  1.242102\n",
       "49   0.793124  0.886069   0.7419  1.257599\n",
       "50   0.793760  0.885434   0.7444  1.223746\n",
       "51   0.793558  0.888524   0.8026  0.882512\n",
       "52   0.797814  0.878890   0.7471  1.181767\n",
       "53   0.796228  0.884263   0.6926  1.347973\n",
       "54   0.795945  0.888531   0.7182  1.175672\n",
       "55   0.796299  0.880470   0.7915  0.945409\n",
       "56   0.797296  0.879239   0.7574  1.071946\n",
       "57   0.796913  0.882267   0.7831  0.942593\n",
       "58   0.801368  0.873740   0.7939  0.919292\n",
       "59   0.800200  0.877647   0.6928  1.462473\n",
       "60   0.798771  0.874061   0.7440  1.227821\n",
       "61   0.800434  0.874688   0.7889  0.988183\n",
       "62   0.799015  0.877008   0.8077  0.906998\n",
       "63   0.801302  0.870423   0.7310  1.388462\n",
       "64   0.800684  0.877583   0.7574  1.181912\n",
       "65   0.802746  0.869008   0.7955  0.949471\n",
       "66   0.801218  0.875713   0.7745  1.019907\n",
       "67   0.803304  0.868892   0.8041  0.910334\n",
       "68   0.801602  0.868704   0.7786  1.007634\n",
       "69   0.805691  0.865346   0.6831  1.544469\n",
       "70   0.803564  0.866611   0.7838  1.024250\n",
       "71   0.806442  0.859832   0.7344  1.242505\n",
       "72   0.803188  0.869104   0.7910  0.966068\n",
       "73   0.805524  0.857300   0.6178  1.853673\n",
       "74   0.803505  0.867852   0.7711  1.041568\n",
       "75   0.806003  0.860268   0.7924  0.963718\n",
       "76   0.806208  0.857898   0.8053  0.922285\n",
       "77   0.804640  0.865436   0.7657  1.043818\n",
       "78   0.802770  0.870000   0.7519  1.123323\n",
       "79   0.805674  0.862033   0.7674  1.026951\n",
       "80   0.806554  0.858804   0.7150  1.343560\n",
       "81   0.806025  0.859585   0.8149  0.876905\n",
       "82   0.806409  0.858396   0.7852  1.038660\n",
       "83   0.805107  0.863573   0.8003  0.895681\n",
       "84   0.808445  0.855561   0.7874  0.981879\n",
       "85   0.805201  0.863064   0.7623  1.194080\n",
       "86   0.807710  0.855929   0.7271  1.133928\n",
       "87   0.806342  0.857340   0.8184  0.872664\n",
       "88   0.808027  0.857785   0.7024  1.409407\n",
       "89   0.806091  0.865420   0.7178  1.358908\n",
       "90   0.809527  0.858298   0.7617  1.140489\n",
       "91   0.810531  0.854347   0.7833  1.015551\n",
       "92   0.808495  0.857574   0.7631  1.142805\n",
       "93   0.809997  0.856164   0.7649  1.206423\n",
       "94   0.808929  0.852210   0.8003  0.881648\n",
       "95   0.809093  0.855015   0.6668  1.583443\n",
       "96   0.810347  0.850783   0.7892  0.967302\n",
       "97   0.810597  0.851840   0.7843  1.030047\n",
       "98   0.814202  0.841273   0.7713  1.155358\n",
       "99   0.810614  0.849188   0.7864  1.029015\n",
       "100  0.810262  0.851756   0.7857  1.026854\n",
       "101  0.812517  0.846413   0.8357  0.825864\n",
       "102  0.815003  0.843260   0.7779  1.073456\n",
       "103  0.810113  0.853025   0.7433  1.244798\n",
       "104  0.813117  0.847161   0.7520  1.161791\n",
       "105  0.811732  0.844111   0.6979  1.430658\n",
       "106  0.812283  0.840641   0.7376  1.223944\n",
       "107  0.811966  0.847477   0.7363  1.208852\n",
       "108  0.810614  0.846170   0.6837  1.454006\n",
       "109  0.811081  0.845101   0.7686  1.047065\n",
       "110  0.812032  0.842180   0.8083  0.907855\n",
       "111  0.813101  0.842753   0.7027  1.507952\n",
       "112  0.815370  0.843071   0.7774  1.011924\n",
       "113  0.813702  0.841153   0.7495  1.240695\n",
       "114  0.811866  0.846218   0.7480  1.242441\n",
       "115  0.812033  0.846413   0.7624  1.160933\n",
       "116  0.813235  0.841774   0.8015  0.939213\n",
       "117  0.816722  0.835592   0.7969  0.970547\n",
       "118  0.815320  0.836566   0.8316  0.817682\n",
       "119  0.813067  0.840776   0.8131  0.871966\n",
       "120  0.813218  0.842877   0.7901  1.042655\n",
       "121  0.815256  0.838986   0.7963  0.949191\n",
       "122  0.814753  0.836679   0.8034  0.926067\n",
       "123  0.815237  0.838148   0.8019  0.979901\n",
       "124  0.814803  0.836516   0.7531  1.067855\n",
       "125  0.814720  0.838348   0.7447  1.180281\n",
       "126  0.864177  0.690799   0.8828  0.674274\n",
       "127  0.881859  0.629949   0.8947  0.628729\n",
       "128  0.885564  0.606451   0.8879  0.635213\n",
       "129  0.887717  0.595411   0.8852  0.636735\n",
       "130  0.890738  0.575221   0.8948  0.594837\n",
       "131  0.895828  0.557261   0.8926  0.613845\n",
       "132  0.896312  0.546317   0.8933  0.599250\n",
       "133  0.895494  0.539884   0.8954  0.566925\n",
       "134  0.898932  0.525957   0.9016  0.533324\n",
       "135  0.901769  0.510789   0.8968  0.564788\n",
       "136  0.901874  0.505591   0.8896  0.587525\n",
       "137  0.903188  0.495221   0.9047  0.517448\n",
       "138  0.903838  0.486583   0.8924  0.564046\n",
       "139  0.904322  0.483997   0.8979  0.543709\n",
       "140  0.904222  0.473070   0.8995  0.520675\n",
       "141  0.906133  0.468450   0.8958  0.532971\n",
       "142  0.903338  0.467556   0.8959  0.532476\n",
       "143  0.907744  0.453686   0.8876  0.545183\n",
       "144  0.905858  0.452628   0.8961  0.510873\n",
       "145  0.905991  0.447842   0.9010  0.507043\n",
       "146  0.907185  0.442364   0.8957  0.506547\n",
       "147  0.908077  0.436945   0.8982  0.492479\n",
       "148  0.908695  0.433048   0.8875  0.552895\n",
       "149  0.908128  0.428724   0.8952  0.511349\n",
       "150  0.907777  0.430519   0.8940  0.497634\n",
       "151  0.908805  0.421326   0.8829  0.552516\n",
       "152  0.909396  0.418479   0.8892  0.534076\n",
       "153  0.905474  0.424919   0.8999  0.494221\n",
       "154  0.910197  0.413875   0.9035  0.480902\n",
       "155  0.908077  0.414236   0.9024  0.466880\n",
       "156  0.911762  0.404789   0.8821  0.552097\n",
       "157  0.909162  0.408600   0.8896  0.512937\n",
       "158  0.908895  0.407933   0.8877  0.522683\n",
       "159  0.909963  0.402576   0.8959  0.481394\n",
       "160  0.909329  0.403852   0.8926  0.502462\n",
       "161  0.909824  0.399309   0.8802  0.533447\n",
       "162  0.911031  0.395514   0.8977  0.484351\n",
       "163  0.911916  0.392960   0.9008  0.469742\n",
       "164  0.910731  0.396264   0.8867  0.516346\n",
       "165  0.910080  0.395962   0.8587  0.636328\n",
       "166  0.910142  0.393212   0.8680  0.582805\n",
       "167  0.911399  0.390882   0.8996  0.478148\n",
       "168  0.912517  0.391586   0.9051  0.443878\n",
       "169  0.913168  0.386341   0.8833  0.541324\n",
       "170  0.911332  0.389770   0.8955  0.483724\n",
       "171  0.910609  0.387938   0.8867  0.504905\n",
       "172  0.910047  0.388518   0.8621  0.631266\n",
       "173  0.912150  0.384813   0.8884  0.477791\n",
       "174  0.912450  0.384980   0.8916  0.537644\n",
       "175  0.912366  0.379531   0.8886  0.526910\n",
       "176  0.913081  0.379505   0.8968  0.467552\n",
       "177  0.911883  0.382069   0.8958  0.493149\n",
       "178  0.912583  0.382355   0.9070  0.442668\n",
       "179  0.913468  0.377196   0.8994  0.468156\n",
       "180  0.912567  0.380442   0.8923  0.519240\n",
       "181  0.914100  0.377247   0.8810  0.571466\n",
       "182  0.914653  0.371651   0.8667  0.593889\n",
       "183  0.914252  0.377110   0.9078  0.442833\n",
       "184  0.912984  0.375147   0.9001  0.467165\n",
       "185  0.912634  0.377772   0.8828  0.526635\n",
       "186  0.914651  0.372653   0.8827  0.545690\n",
       "187  0.913885  0.374215   0.8510  0.691880\n",
       "188  0.915070  0.372565   0.8922  0.534846\n",
       "189  0.929339  0.333017   0.9151  0.414215\n",
       "190  0.937250  0.310269   0.9165  0.412581\n",
       "191  0.939354  0.303018   0.9152  0.414688\n",
       "192  0.940320  0.299536   0.9150  0.416509\n",
       "193  0.941856  0.295448   0.9174  0.404975\n",
       "194  0.941973  0.292452   0.9154  0.415815\n",
       "195  0.941656  0.291321   0.9217  0.398174\n",
       "196  0.943012  0.290103   0.9196  0.406090\n",
       "197  0.943909  0.286588   0.9190  0.400816\n",
       "198  0.944459  0.282397   0.9166  0.421173\n",
       "199  0.943341  0.284923   0.9201  0.398093\n",
       "200  0.963300  0.229079   0.9320  0.345002\n",
       "201  0.966680  0.222764   0.9310  0.351142\n",
       "202  0.968100  0.217228   0.9323  0.346746\n",
       "203  0.968160  0.215706   0.9330  0.347083\n",
       "204  0.969740  0.211427   0.9326  0.351246\n",
       "205  0.969820  0.211918   0.9309  0.350650\n",
       "206  0.970400  0.208864   0.9318  0.352849\n",
       "207  0.971040  0.205855   0.9308  0.358757\n",
       "208  0.971520  0.203902   0.9320  0.356844\n",
       "209  0.972300  0.201872   0.9357  0.357375\n",
       "210  0.973040  0.199459   0.9310  0.360102\n",
       "211  0.973740  0.198549   0.9327  0.362790\n",
       "212  0.973760  0.196791   0.9316  0.357505\n",
       "213  0.974320  0.194484   0.9321  0.357628\n",
       "214  0.975820  0.192466   0.9319  0.358587\n",
       "215  0.976320  0.189446   0.9302  0.363701\n",
       "216  0.974380  0.191858   0.9329  0.358301\n",
       "217  0.975440  0.189139   0.9339  0.358287\n",
       "218  0.976500  0.186283   0.9307  0.365875\n",
       "219  0.976420  0.186819   0.9325  0.362609\n",
       "220  0.976960  0.185535   0.9325  0.365964\n",
       "221  0.977420  0.184798   0.9322  0.365393\n",
       "222  0.976780  0.183331   0.9331  0.368212\n",
       "223  0.978380  0.181206   0.9297  0.379115\n",
       "224  0.978560  0.178436   0.9311  0.376638\n",
       "225  0.977480  0.180566   0.9321  0.376282\n",
       "226  0.979800  0.176131   0.9307  0.379610\n",
       "227  0.978380  0.178882   0.9325  0.372252\n",
       "228  0.980240  0.173107   0.9329  0.372573\n",
       "229  0.978860  0.176961   0.9304  0.373105\n",
       "230  0.979860  0.172621   0.9307  0.374098\n",
       "231  0.979480  0.173957   0.9315  0.374350\n",
       "232  0.981520  0.170646   0.9333  0.373972\n",
       "233  0.981580  0.169244   0.9315  0.381402\n",
       "234  0.981640  0.167597   0.9289  0.386560\n",
       "235  0.981340  0.167219   0.9298  0.386616\n",
       "236  0.981940  0.167041   0.9338  0.372294\n",
       "237  0.982400  0.164932   0.9301  0.380137\n",
       "238  0.981160  0.167158   0.9300  0.376325\n",
       "239  0.982420  0.163876   0.9304  0.385472\n",
       "240  0.981720  0.164240   0.9319  0.382230\n",
       "241  0.982300  0.162944   0.9285  0.391983\n",
       "242  0.982700  0.163842   0.9320  0.379650\n",
       "243  0.983640  0.159734   0.9305  0.383228\n",
       "244  0.983000  0.160271   0.9301  0.388591\n",
       "245  0.984500  0.157771   0.9296  0.387852\n",
       "246  0.983780  0.157502   0.9321  0.376884\n",
       "247  0.983600  0.157877   0.9330  0.376353\n",
       "248  0.983740  0.157962   0.9326  0.380552\n",
       "249  0.983400  0.158030   0.9323  0.384491"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "better_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEuu6xzUZT90",
    "outputId": "3bc4650f-0740-4064-f288-8ea4c4dc9dc9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzsnXecVNX5/99nZ2d7byzbWJAOIghiVxRNsGFXNCbRJJJm1MQU/X7zS0xiEpMYoyZqvvZoLCGWhESQaAQrKCiIVOlsAbb3nZ12fn+ce3dmZ2eXXdjZ+rxfr33N3HvPvXNm0PM5TznPUVprBEEQBAEgaqA7IAiCIAweRBQEQRCEdkQUBEEQhHZEFARBEIR2RBQEQRCEdkQUBEEQhHZEFIQRgVKqWCmllVLRPWh7vVLq3f7olyAMNkQUhEGHUmqvUsqtlMoKOb/eGtiLB6ZnHfqSpJRqUkotH+i+CEJfIqIgDFb2ANfYB0qpY4GEgetOJy4H2oBzlVK5/fnBPbF2BOFIEVEQBivPAF8KOv4y8HRwA6VUqlLqaaVUpVJqn1Lqx0qpKOuaQyl1j1KqSim1G7ggzL2PK6UOKKXKlFJ3KaUcvejfl4E/AxuB60KeXaiUetnqV7VS6k9B125USm1VSjUqpbYopY63zmul1Pigdk8ppe6y3s9TSpUqpX6klDoIPKmUSldK/dv6jFrrfUHQ/RlKqSeVUuXW9X9Y5zcppS4Kaue0fqNZvfjuwjBGREEYrKwBUpRSU6zBehHw15A2fwRSgXHAmRgRucG6diNwITALmANcEXLvU4AXGG+1+RzwtZ50TCk1BpgHPGv9fSnomgP4N7APKAbygResa1cCd1rtU4CFQHVPPhPIBTKAMcBizP+7T1rHRUAr8Keg9s9gLKtpQA7wB+v803QUsfOBA1rr9T3shzDc0VrLn/wNqj9gL3AO8GPg18AC4HUgGtCYwdYBuIGpQfd9HVhlvX8T+EbQtc9Z90YDozCun/ig69cAK6331wPvdtO/HwMbrPf5gA+YZR2fDFQC0WHuWwHc0sUzNTA+6Pgp4C7r/Tzru8Z106eZQK31fjTgB9LDtMsDGoEU6/hF4IcD/W8uf4PnT3yTwmDmGeBtYCwhriMgC3BiZuQ2+zCDNJjBryTkms0Y694DSin7XFRI++74EvAogNa6TCn1FsadtB4oBPZprb1h7isEdvXwM0Kp1Fq77AOlVAJm9r8ASLdOJ1uWSiFQo7WuDX2I1rpcKfUecLlS6hXgPOCWI+yTMAwR95EwaNFa78MEnM8HXg65XAV4MAO8TRFQZr0/gBkcg6/ZlGAshSytdZr1l6K1nna4PimlTgEmAHcopQ5aPv4TgWutAHAJUNRFMLgEOKaLR7fQMZAeGrwOLWd8GzAJOFFrnQKcYXfR+pwMpVRaF5/1F4wL6Upgtda6rIt2wghEREEY7HwVOFtr3Rx8UmvtA5YAv1RKJVt+/u8RiDssAW5WShUopdKB24PuPQD8B/i9UipFKRWllDpGKXVmD/rzZYwrayrGZTMTmA7EY2bdH2IE6W6lVKJSKk4pdap172PA95VSs5VhvNVvgA0YYXEopRZgYiTdkYyJI9QppTKAn4Z8v+XAQ1ZA2qmUOiPo3n8Ax2MshFALTBjhiCgIgxqt9S6t9bouLn8HaAZ2A+8CzwFPWNcexfjwPwE+prOl8SUgBtgC1GJ866O764tSKg64Cvij1vpg0N8ejKvry5ZYXYQJYO8HSoGrre/yd+CXVj8bMYNzhvX4W6z76oAvWNe64z6MEFVhgvKvhVz/IsaS2gZUALfaF7TWrcBLGLdc6O8ijHCU1rLJjiCMNJRSPwEmaq2vO2xjYUQhgWZBGGFY7qavYqwJQeiAuI8EYQShlLoRE4herrV+e6D7Iww+xH0kCIIgtCOWgiAIgtDOkIspZGVl6eLi4oHuhiAIwpDio48+qtJaZx+u3ZATheLiYtat6ypDURAEQQiHUmrf4VuJ+0gQBEEIImKioJR6QilVoZTa1MV1pZR6QCm1Uym10S4hLAiCIAwckbQUnsIU6+qK8zA1ZCZgSgE/HMG+CIIgCD0gYjEFrfXbh9k28WLgaW1yYtcopdKUUqOtui29wuPxUFpaisvlOnxj4bDExcVRUFCA0+kc6K4IgtDPDGSgOZ+OpYpLrXOdREEptRhjTVBUVBR6mdLSUpKTkykuLiaoFLJwBGitqa6uprS0lLFjxw50dwRB6GeGRKBZa/2I1nqO1npOdnbnjCqXy0VmZqYIQh+glCIzM1OsLkEYoQykKJTRsd59AYFa+L1GBKHvkN9SEEYuA+k+WgrcpJR6AbNJSf2RxBMEQRCGOm6vn/01LdS2uEmNd/Lezipiox3kpsYSG+0gJjqKGEcUBenxZCbFRrQvERMFpdTzmL1ls5RSpZhNQJwAWus/A8swO2rtxOw6dUP4Jw1+6urqeO655/jWt77Vq/vOP/98nnvuOdLSutogSxCEwcqB+lZykuPYW92M2+tnbFYie6ub0Rpe/riUvdUteHx+8+fVtPn8VDe1kZbgJDclji3lDeSkxFHb4qakpgV/D8rQ3XXJdK47aczhGx4Fkcw+uuYw1zXw7Uh9fn9SV1fHQw891EkUvF4v0dFd/8TLli2LdNcEQeiGNq+PstpWkuKiyUmOw+Pzs/VAAy1uH00uL+X1rdS1eEhLcPLujir217SQEudEKfhgTw0pcdE0uMx23EqBXV/U6VAck53UPsN3OqJIjXEyNjOB8joXOyuamFOcQVVTGwXpqVx8XB7FWYmkJ8ZQ1djG3LEZRClFdbMbt9dPm9eH2+tn4qjkiP8mQ67MxWDk9ttvZ9euXcycOROn00lcXBzp6els27aNzz77jEsuuYSSkhJcLhe33HILixcvBgIlO5qamjjvvPM47bTTeP/998nPz+ef//wn8fHxA/zNBGF4UNvsZndVE7sqmjnU4KLN62dPVTOrtlfQ7PbhiFJMz0/ls4ONtHp8YZ+RmxLH9PxUKhtdVDW5uWX+BEprWxmfk0R6gpOyulYmjErG7fVz6vhMRqce/f+/hRkJh2/Uxww7UfjZvzazpbyhT585NS+Fn17U9Z7ud999N5s2bWLDhg2sWrWKCy64gE2bNrWndD7xxBNkZGTQ2trKCSecwOWXX05mZmaHZ+zYsYPnn3+eRx99lKuuuoqXXnqJ666TTbEEoae0uL28s6OK+lYPh+pd7KlqZndVM3uqmqlv9XRoqxTkpcZz0XF5nFCcwfZDjazdW8PVJxRyQnEG6QlOEmKjyUuLIzXeSUVDG3lp8Tiihn8SxrAThcHA3LlzO+T4P/DAA7zyyisAlJSUsGPHjk6iMHbsWGbOnAnA7Nmz2bt3b7/1VxCGKm9uO8Qzq/exqbyBhlYPbV5/+7XRqXGMzUrkwhmjGZuVyLjsRMZmJZGfFk90lCKqFwP8QMzYB4phJwrdzej7i8TExPb3q1at4o033mD16tUkJCQwb968sGsAYmMDGQUOh4PW1tZ+6asgDFXuWbGdP63cSX5aPPMmZpMa7+TsKTkUpieQmRRDQsywG976BfnV+oDk5GQaGxvDXquvryc9PZ2EhAS2bdvGmjVr+rl3gjD8qGl286eVO7nouDx+f+VxxEQPiXW4QwIRhT4gMzOTU089lenTpxMfH8+oUaPary1YsIA///nPTJkyhUmTJnHSSScNYE8FYXiwoaQWgC+cWCSC0MeIKPQRzz33XNjzsbGxLF++POw1O26QlZXFpk2BCuPf//73+7x/gjCcWL+/jigFMwpSB7orww6RWEEQhhwbSuqYlJsicYMIIL+oIAhDAr9fU1bXSm2Lmw3767hoZt5Ad2lYIqIgCMKgwuXxsWp7BRtK6vFrTYwjig/2VLOprKHDwrKZhVIeJhKIKAiCMChYub2C/3trFx/tq8Xj00RHKaIdCpfHz7S8FBbNLWTiqGSS46LZXdnMBceOHuguD0tEFARBGHDe2HKIrz29joL0eL5y2lhOG5/FyeMyiXZE4fPrnq0k9vsgytH5fFMlLP8hTLkIpl/W9f0tNVCzB/Jmdn5OV8/uCo8LoqLBETTE1uyGpFyI6cVCOK1B+3v32UeJiIIgCAOK1+fnV8u3Mi47kdduOaNTimlYQdAaXr0N/F5oa4D9a6DxAMSlwvhzIS4FvG5IHgUbl0B9CWz5J6x9DOr2w6jp4Ko3g23+8dBUAZ88b55ddLIZ0Gv3QnQcaB/U7oNZ18HEz8PO/0LdPsg9Fna8DtU7IWsCnPhNcDdD2TrYstT0Ye6NcPJNsOklWHozZE+G838HDidUfQZ734Vjr4DK7XDwUxh/DqQXQ0wSfPAwbF8OTYfM90rIhLP+17SPICIKA0BSUhJNTU2Ul5dz88038+KLL3ZqM2/ePO655x7mzJnT5XPuu+8+Fi9eTEKCmXlIKW5hKPLPDeXsrmzmkS/O7vmag9K1sO5xcMSaAfOYs8xg2lAOW5ca0XA4zew/fzZc8hC8eRfUlRhLoGqnGWS9bfD+n0BFmcE7tcC0S8iE4tPA6zLPKpgLH//F/DkTIWU07HwD8ufAnK/C1n/BP60qyfHpZuBuPGietepuI14Fc6FyGzx1fuB7RMcFxMiZGHgPEOWEaZdCxlhorYWWatOvCCOiMIDk5eWFFYSect9993Hddde1i4KU4haGIss+PUBBejznTh11+MY2n7wA0fHw/c8gNtlUuLO56H5AmXOe1oC75isrOrazaaowLprkXHM8d7G5PypEoOYuBk8LFJ4I0THQ1mg+G+Ccn0L1LkjIgKRRgc/Z87aZ7Y+aBsdeaQb2g58aEUrJM0L25l2QMQ5mXw8VW4yYNFXAmFMg85ie/yZ9hIhCH3D77bdTWFjIt79ttoe48847iY6OZuXKldTW1uLxeLjrrru4+OKLO9y3d+9eLrzwQjZt2kRrays33HADn3zyCZMnT+5Q++ib3/wma9eupbW1lSuuuIKf/exnPPDAA5SXl3PWWWeRlZXFypUr20txZ2Vlce+99/LEE08A8LWvfY1bb72VvXv3SoluYVDR4vby7s4qrplbFH4b2OYq+Pv1ZrY//TLImggo446ZcqFx0YQS7H8P9t93tc1sUk7X9wdTMLvjcWzQ3gbRsTBqaud7xp5h/mxS8sxfMAt+HXg/+jjzN4AMP1FYfrtR4r4k91g47+4uL1999dXceuut7aKwZMkSVqxYwc0330xKSgpVVVWcdNJJLFy4sMv9jx9++GESEhLYunUrGzdu5Pjjj2+/9stf/pKMjAx8Ph/z589n48aN3Hzzzdx7772sXLmSrKysDs/66KOPePLJJ/nggw/QWnPiiSdy5plnkp6eLiW6hUHFOzuqaPP6uWCM3wz0o2dC2hjY9GLAz75/tfG9v3cfxGcYF4+rDo7/0kB3f1gy/ERhAJg1axYVFRWUl5dTWVlJeno6ubm5fPe73+Xtt98mKiqKsrIyDh06RG5ubthnvP3229x8880AzJgxgxkzZrRfW7JkCY888gher5cDBw6wZcuWDtdDeffdd7n00kvbq7VedtllvPPOOyxcuFBKdI9AfH5NlKLLCQlAfauHlLjoDm1cHh/r99exq7KJcdmJnDwus/36oQYXJTUt1LV4qG1x4/FpZhSkUtviZtuBRtw+P0mx0azcXsH+mhZiHFFMyk1m7tgMWt0+Xllf1p5auiBuE3Ne+QLQxX6U5/4Cpi6E8vWw7kkoXQdXPNFxBi70GcNPFLqZ0UeSK6+8khdffJGDBw9y9dVX8+yzz1JZWclHH32E0+mkuLg4bMnsw7Fnzx7uuece1q5dS3p6Otdff/0RPcdGSnQPD/x+zcEGF40uLznJsaTGO9lV2cSOiiYK0uMpTE/g4bd2MaswjT+88Rken+ams8aTGOtgc3kDqfFOvH7NtgMNfLinhvJ6F/lp8cwoSCUtIYayulY+2F3dYX+C7ORYjslOJCEmmlXbK3q0p/C4rEQmj06mzePn3R1V/HNDOQDHFaTS5vUzoyCVnzr2oEpS4LqX4OBG41MfNQ2KToKSD2Hyhca/n15sAq9et/HpCxEhoqKglFoA3A84gMe01neHXB8DPAFkAzXAdVrr0kj2KVJcffXV3HjjjVRVVfHWW2+xZMkScnJycDqdrFy5kn379nV7/xlnnMFzzz3H2WefzaZNm9i4cSMADQ0NJCYmkpqayqFDh1i+fDnz5s0DAiW7Q91Hp59+Otdffz233347WmteeeUVnnnmmYh8b6GP8PuNz9uaiVc1tfHmtgpOGpvJn1buYMuBBi6ckYfPr1m/v5a1e2s77CYWpWgfpKMUFGUksLe6BYDUeCcZiTHc9vdPgI57CWcnxzK3OINrcpPZXN7A9oONNLV5SU+I4Zq5RcwvhLFjxrBmTx2rd1Wzp6qJ0tpGbjx9HKeMzyI9wUlafAwazYaSOmbtfoT04hn4Jy+kpsVNcWaCsS6W/wid4aL8jLvx+vyMyQzsOcKjP4LRM6DwBPMXzNSFnX8rEYSIEjFRUEo5gAeBc4FSYK1SaqnWektQs3uAp7XWf1FKnQ38GvhipPoUSaZNm0ZjYyP5+fmMHj2aL3zhC1x00UUce+yxzJkzh8mTJ3d7/ze/+U1uuOEGpkyZwpQpU5g92wS1jjvuOGbNmsXkyZMpLCzk1FNPbb9n8eLFLFiwgLy8PFauXNl+/vjjj+f6669n7ty5gAk0z5o1S1xFgwmPC5erhfUVmtqGRk59dT4vx1/G/7UtoLbFjdev8fl1+wBelJHA3cu3AZpxWUksmJbLsQWpZqvIxjaqmto4JjuJiaOSuOc/n1Gxaz3vjV9FY1w+SWd8i5y8YnZWNOHx+ZmUm0yr20e0Q5Ec5+y6jzvfgGevhEse5orZi7hidkG3X2lMVCX84z7YngoTzyA1KztwccfrqJpd5M+40qR62vi8cGizSesUBgVK6x7YgEfyYKVOBu7UWn/eOr4DQGv966A2m4EFWusSZZyV9VrrMOkEAebMmaPXrVvX4dzWrVuZMmVKX3+FEY38phHE56Hpz+firdzFpW0/JU9V8WzMrylzFHD/pKc5yb2G0a7dRJ/8DZ7e2MTMwjS+cmoxTZ++StLy76AuvBfGnGoWWCVkdH58yTrU0wuJUlHgboKTvtkxwyUYrc2CrvcfgEXPQ+50c76hHB462QrofhkWPnD47/XmL+Ht35l+HXc1XPxg4DN+lWfSOfOOh8WBCQwV2+ChE+HS/4PjFvXyhxR6g1LqI6111wufLCLpPsoHSoKOS4ETQ9p8AlyGcTFdCiQrpTK11tXBjZRSi4HFAEVFRRHrsCD0GYe2QM4U46tpa6SqbCcPfOrkUG0j1zQ8wbya9bQQx6vp9+IqOB0+g3xfKb+tu80EVAHeeIMTxs2DgqvgYAPJS78GvjZ45RtmoB19HHz5X7Dt35A3qz2n3fHOPSYVc/Fb8NJXzUIvG58X3r8fJnzOZNWt/ysss/bv2PgC5N5l3u943QhCWhEc2AAHN0FzpVkkFkr5BvjXLVC7B8bPh5ypRmRm3wAFc8zKYU8LpBRA+cdmAVlaobnXzhTMPbbP/wmEI2Og91P4PnCmUmo9cCZQBvhCG2mtH9Faz9Faz8nOzg69LAiDi10r4eGT4YM/A+BdeitZT8/jqo+v49f7rmFezRLeST4P/xdeJqGlnIzPXjB5+FFOIwjn3GkWWjli4JO/GRFYfrvJi//GuyZXP2silH4IS75kBv4/Hg+b/2EG3B0rzOw+Nd8898BGE5wF2PBX+O/P4dH5sG2ZaZtaBMecbY5tz0HtHtOfKQuNwL3yDXj+GrOoyqZ8vXn2iv81C7fiM+CU78CZPzQ1fpZ938RKGg+Y9jOuNK/73gs84+AnZlVy1sRI/osIvSCSlkIZUBh0XGCda0drXY6xFFBKJQGXa63rjuTDtNbdptwJPSdSLsURwwZrF77//oK90WMp3PwyH/onMyknntRRs+C4RZw+4XPGipj9ZfjoKTj2Khh3FvjccOqt5tpNa02dnb9eBvX7YcFvTFbON94xZQ9+P8UM6lMvNn75NQ8Zt5LW5rlgZuqr/wRv3GlWy1ZsMS4cTwus/KVxE006z9T/efU2U48ne5Ip3pZWZETF74FD1oz+/Qfgc3fBe/ebZ2oNaDjvd3Di4sBvcPb/wtLvGAHwWYI0/hyTUrr3nYCrqMqqG+ToJrYh9CuRFIW1wASl1FiMGCwCrg1uoJTKAmq01n7gDkwmUq+Ji4ujurqazMxMEYajRGtNdXU1cXFxA92VoUlbI2z9F97xn8e7+x2K/301Xhx4Lv4/UmfP7Nz+nDtNuYYZV4WND3DM2SYfv3q3KYNgE59uZt6fvgQL7jYLv/7zYyj7yKRtpllu1nzLhbzmQTPz93vhiieN2+a1H5lrY041bqFXb4PPXrNEYY+puZNn9VlFwbh5sPZxOOUWeP2nxgWVlGMEJLhvANOvMBbE+r/C2NPNuZR8E2Te806gXUNZ5xW+woASMVHQWnuVUjcBKzApqU9orTcrpX4OrNNaLwXmAb9WSmngbeDbR/JZBQUFlJaWUllZ2Ue9H9nExcVRUNB9ponQBdtfA28rPzg4n42uz/N/o16mYOLxnBpOEMAM7t2trVEKFj1nCrc5Q4R6wW/gjB+aQXXGInjjZ+Z5598TaJNaYGrxNFfB19+CuDTjVsocDyvuMDV/ik+zyi/kG4tDa1MhtPBESB9rnjn6OJjzFdj1Jmz5B6DNiuIpF4bvd0yCKQq34XlIsly+yaOh+HQTA7l3GlzxuHEt5c3q6a8r9AMRXaegtV4GLAs595Og9y8CR14RzsLpdDJ27NijfYwgHD0NZpnNsqoc7vvCSYw/tg9SLWOTO9bZsYlJCNT2ScqGq/9qRCAxqJKmUnDad837UdMC55NHGSukehekjzHnsiYY91FLjSlHnTHO3H/tEiMs2gr3bXrJvOYcJjtt5nWw7gn4+BlT3dMZZyyi+hJY/aBJeW2uFEthkDH8VjQLwkDibQMgJjaud1U/+4JJC8KfP+mb4c9f+gh4mgPHWRPNzL52jznOsCZahWa9C36/Ke+8f7UJDqeN6b4/+ccb66DxAIyysosSMuDzv4TNr8C+1eZcsuygNpgY6OwjQRheeF24iWZ2sdk1bFCTmBmIPYARBXcj7HvfHKeHWN9RUZBjLcLMmtBxV7FwKGWC2GD2HwgmfWwgVVYshUHFIP+vVhCGFq2tLbi0kxPHRn4zlD4nc7x5/ew185oexhLIscpDZ0/q2TMnWRvKhFoDGcVmzUW4a8KAIqIgCH1IRU09bTg5cVyYTKLBjr1WYN97JhXVGWafjVHWiufsHq52Lz7dDPqjQ6r6ZowLvBdLYVAhMQVB6EOaW5pxEMPU0d1WaxmcpOSZmIGn2ayVCIc9uIfbUCYczji4dVNnV5PtmnLEmuwmYdAgoiAIfYivrQWvchLn7GL3rsGMUmawb62DyReEb1N0silxPe7snj83XOzBDmKnjO56RzRhQBBREIQ+xO9x4YuKPXzDwcrlj5mFal1tSamUWZl8tNiWQrK4jgYbIgqC0Idorwu/YwiLQnpx/3xOfJpZuyDxhEGHiIIg9CHK2wYxQ1gU+pPLHzOrqIVBhYiCIPQhytcG0RI47RHH9CIuIfQbkpIqCH2Ex+cn2u8mKrRGkSAMIUQUhOHJ278zG8P0IzXNbmJx47DrEQnCEEREQRh+uJvhzbvg0yV990yfFz54BDyuLptUNrYRqzw4Y8Ms+hKEIYKIgjD8aLZKqLfU9N0z970Ly38Am1/uskl1s5tYRBSEoY2IgjD8aK4yr621fffM2n3mdf+aLptUNbYRi4e4eHEfCUMXyT4Shh/2PsJ9aSnU7Tev3YhCdbMRBUQUhCGMWArC4MLrhr99ESq2Hfkz2t1H1X3SpUaXh8qSHeagajs0h39ulRVTiBH3kTCEEUtB6H9K1sLqP8LlT3Sui1NfAluXmv2A7dr9PeCfG8qoaGjjq6eNJcoShYaaQ9y7dDM/WjCZ9ftrmV6QavaZB1Liotv38251+1i/v5bH3t3DrsomLj4uj6vnFpGZGMPH+2r5n1c+5XeNW4lX8SSpVla/9SqPV07hk9J6UuOd1Da7cTqi8Lhb+B9ARUtKqjB0EVEQ+p89b8GWf8LZeyFrPNoaqZVS4G4ybTytgJml/3vjAXZWNDF/cg41LW68Ps2EUUlUNblZs7uanRVNvL7lEABvbD3E11s2czYQ72vkqff38M8NZdS2eDp0ITMxhllF6VQ0uthS3oDXr0mNdzI9P4U/rtzJA2/ubG+bnRzLtIR6tsecwrT6VWx4/z98FJPLWZNzaGnzkTE2Bo/Xj7e5DvYCIgrCECaioqCUWgDcDziAx7TWd4dcLwL+AqRZbW639nUWBpqPn4aGAzDvR1022VnRSGFGApvLG2h0eTljQhbl9S5Gp8TxWUUjm0prSXP6OWPaGGKio9hX3cz7u6qZvKucWcAb776HY0oKv31tO7HRUZx/bC7ln3zIncBf393Gqh3r2F3ZxO6qZhxRisff3dOpD06HIjMxlq+fMY68tHgeeXs3bpcRCKfy8fUTs1m2o5n/OX8KZXWtJMZEoxRsLK1n64EGRiU7eTvjLqpnLGbsmdeRFBtNSU0Lyz49gMfnZ3xOEqeNTSHhdxXMOmkuns2H+HJ8Izd8cX7nSqiNh+D3QLSUuRCGLhETBaWUA3gQOBcoBdYqpZZqrbcENfsxsERr/bBSaiqwDCiOVJ9GLHvfg4I5XQ5WWmu0hqiooBLGax4GTwv6zB/i9vlZua2C3NR43FtfI3bfSv475ns88OZOMhNjqG52AzAuK5HdVc0UpMdTVtfK9x0vcELUGk5a+hCnjs9i+acH8Po1P4kuY1Y0rF77IY+vySI9wYnPr/nVsm1clVoHQIxu4+P9tUQpxXNfO5GZRWn8Z/MhRqfGkWgN3CnxTmYVpZEQE/jP+MunFMNf/gCWftxxZjZ3XHpi179NUyXcs5k8tQdizXMKMxL4+pnH2D8OVO0ANKQV4cyfiXPHCoiOgsaDZgP6cWfCMfPBa6wbsRSEoUwkLYW5wE6t9W4ApdQLwMVAsChowN6NJBUoj2B/hh+HtkArqDmGAAAgAElEQVRiFiTldN2mvgyeOh//xQ+jZl7T7ke38fk1P/j7J7y2+SCnjc/ipHGZbNm9n3sqttAYlcrxP15ObLSDpjYvAA84n2KhYzXX7jqXz00txq81xZmJOByK1zYd5Dtnj2fd3loumD6Kr2/+gOiWCk4vimf55oNcMGM0t54zkfy3l8FG+N7sKL7Y8hopc67ClTuHPVXNnOJqgxfhquOyuPics/H5dfugf8ksq3ja3veYXv1u11ZMc5XZvMXXBi210N0maA1l5tVyV3Xitdvhgz+b92lFkNsAG/4KTYfg1dtg27/h/Qfg8sch19qcXiwFYQgTSVHIB0qCjkuB0CnbncB/lFLfARKBsIXalVKLgcUARUVF4ZoMP3a8Aat+DTcs63qQefYKE5C95KEOpxtdHnx+TVpCDDQdBODRZe/xwn/zOH1CFnFOBxUNLjaW1dPm8VNW18r8yTl8WlbPf7Yc4uIEUx4izt/EojmFePyaK0YdoDS6iDM/aIBaeOy8ZE48fXYH6+KO84K2aNz7Hqw1bpz7LxjFfZmnBgTJbwbgxD1vkNhYDlEVcO3fyEuLh4+bTRtPK7HRXdT0X/9X2PKPbkSh0mwteehTaD1MWmqDNQ/xNIe//umLgfdpRYD1Hd76rRGEM34Ib/8WaveazexBLAVhSDPQgeZrgKe01r9XSp0MPKOUmq619gc30lo/AjwCMGfOHD0A/ex/9qyCsnVQsQXyZnW6rP0+VOMB/GXr+eMbO1i7t4YLZozG7fVzz3+24/L4mJybwviG1fwBiPfUMSo7llfWl+H2+pkZX8UxhVNwKMUDRW8xW29B376E0tpW8j9aB++BEx+/uNAa6H59NnNOuQmaTb7+ycmHINjdpDX4PBAdY443vxK41lCGsgdMAHeLeW20BuSd/zULzeLTTYkK6HrmDiZDyesynxm6a5ffZ1JRx55pROFwaamHsxScCZBaCBMXQEoBxKWa8+seN5vYn/EDY0k0V4LX2oheREEYwkRSFMqAwqDjAutcMF8FFgBorVcrpeKALKAigv3qH3weswH6uHlHdr+1WEqXf8JmPY7CjATqWzys3F7BjopGXl+3hQ+i/fgqt/Onki1kpSZxx8ufAnB8URozCtLYUdHInDigDC6eGMeXrj3ZPPvgJvjzIjjzNSg4AX5hShgrpSjMSIDSDwP9aGuAtkbwe2DbMnA3mvOh6wg2vQTLfgC3bTOWza43zebulVsDs3Gb4Fl5XCq46s2zZ30hKPuopZvfZh9oP/jc8O4f4NgrIdOKAbTUmGvZkwLH3WH3zR3m8/w+I1ynfAfOuTPQ3/Ric99ljxgRTMw2C+a8Vl0kcR8JQ5hIisJaYIJSaixGDBYB14a02Q/MB55SSk0B4oDKCPap/9j2Kvz9y/DttZA9sde3e2v2Ew0sf30F36rPJic5lha3j6Y2L9FRiq9MjIG9Zjb/ylVZTJt1ClsPNOJ0KMZlJ+GwZ/FrNkAZpOqGwMPt2XHFlo6lIPw+sw1jxWaITQkIQn2puV613WqozL1Ae+J/xRbjqnE3m1l3zS7jWqncGvg8m+ABeM5XjKBs+3dHUfB2UXjO5w0M5FWfGRfbql/Dbduheicst1xKmceYbSWPxn3UXAl+b+eNYM75mbFQ7BhCYrZYCsKwIWKioLX2KqVuAlZg0k2f0FpvVkr9HFintV4K3AY8qpT6LibofL22k9aHOrbbom7fYUXhwMY3yfjnddxR8DSf1jo5Z+oovl6xhzRgnHcX/3v+FF76uJTCjGh+e8UM8lLjiS9fDU+Z+6c7SkAppualdH64PSja9YAAXA2Bvu18I3De3QyOGCMUecdD+cdGGEIH9aKToWKref/YOWbPXvv7+txQaVkRY042Wy7Wh9zvaYHRx5nZ9fQrjOjY5SPCuY/qSuCZS+CaF8yA6/da3y1I0P7z/wANNbvNswtOMO6ow1oK3biP7GuhojDtko7HSdkmQ0ksBWEYENGYgrXmYFnIuZ8Evd8CnBrJPgwY9uAWOqBiMn4cUQqtNX94/TPcb/+d26Obaa3cQ1rKNJ5YtZUfxdXhV9FMVvuYfGoRXzvdbHTeHqwNHuQPdbNvgD1YB/vW2+rNa91+KF0bOO9pAZdJCSVrohEFV0PHQT3KCRPOhf/+zAy41TsgOdfMysGIQvl68370TLMHb6j7yN0MRSfB1982xzlT4dO/GzdSOFHYvcpYAZ+tgPzjA+dtUUgaBduXgcMJUy+GS61sofiMHsQUgtxHbY3md82wNpW3v3fqYbaMTMwxgXXbUnBKmQth6DLQgebhi+0GsQad+hYPm8vreW9XFY++s4er5xRS0ehixeZDPDu6GWrh4cuPQY87mRdXrIQ1oMadCbv+C1XbUaOmmed5WmHrv8wACpCUawK10y6F/Nmd+2HPlINnzPa9ZR8bt0fuDDi40QzIdrus8ea1rREaSs0A63ObWXPGuMB3c7eYGb/DCjB73ea56cWQkGHah7MUYhIDxzlTzWvFtk4rmoGAyJStM64am1ZLwI5bBO/db96PD0pgs906XaF1kPuoBd69zwSQf7AboqIC1w63j3BSjrHI2qx4i1gKwhBGCuJFiraAKFQ0urjggbe59rE1PLhyFzPyU/nrB/t4b2c1P/j8JE7JtGbHrgaUUlw53ir7MPl8c/7Q5sBzt/4LXr7RBHIB5t5oXBePnQtVgdIM7djuI3djYCYb7D6CgJh4WtpTWMm0soXaGox7J60Qplxk9tWNTbKeU28C0M2VHd1H5RsCGVMpeWFiCs0mq8cmx0plrdgSZCkExR0ObDCvpR8FqpVCwKqZ8DnjplJRHff9TSs0rqeuaK0NLDjztEDjAXPO7m9DqVnvkJDZ9TMgIFT2fRJTEIYwYilECmvG+/76T7n+w5X83PE484uaOHTx35ien8qB+lZS4pwkxkbDH63B2Z7B11sD2bizjLvGDuqCWTQFsO99iEuDM74PM78A9x8Hax6EC//QsR/BFkJLtRmk2xo6tsmfDR89aWb9jZYoZFlxkLZGM9PPPCbgltm32rw2W0lizVUBl4mnFer3w0wrpyAlzwiTp9XEDVILO1sKqYUQk2TiFG0hgWafx2RLxaaY55atC9xnu49ikuDkm6B2j7FObNKKzDoDn7dz4T0IWALJeebfy/79qz4zgtJQbvofmvYair140BYgsRSEIYxYChHA5fHxyW6TsZPvqOXauYVcnrCe7OqPmJ5rZsijU+ONIPj9gdmvPSjVlYByQNoYsyDqUJAo2DNyV51ZzQyQMtq4UDY81zHWAGbgjEkOuTdIFGJTIdNyFXmajStIRQVcRC4r0BzsQrEHdPuz3I3QYr23BccWiZQC81r1GTx/Dbz5C+t6kKUQFWWshXCWQsVWszL5uGvM8c7/Bn23usBnnf49WPjHjt89bQxon5nxh8O2otIKzefafa+yymTXl0FqQfh7g7EtBVvMxVIQhjAiCn2B3wdrHwefhzavj6/9ZR1V1WbAKYqu5c5T43C6qoxrpeqzjvc2HTKDHgQGpfpSM0N1RBt/u53pAx0H/YSswPtZ15nZdckHHZ/fUhOID9ii0NZgRAdMeWp7o3m35T5KzAZnnBncGspM+9QwotAUtJzEXm8Y6lcvshax//cXxlVTu7fjM2zaRcGOKViWgu06mv1lYy3EJMCJ3zDnbPdRV4NwmrX6PdjlFIxtlSTlGPGw90mo+szEG2r3Hj6eAJ1FwY6vCMIQREThSPlsBVRaefv7V8Or3+PO+/7IKb9+k3d3VnFsthl0VVsD7FgRuO/gpx2fY/v1IWAptNYG3CA5U4zbxJ7dB2fTJAaJgj0ANh4InPO4zOzfjg8EWwr24q7syeC0BmhPi6n0mTTKHMemBAQpnKVgu7KCsUXBHhgzxkHONNj5uvV9rQHaGbI7WcY40z87MOxrM2Jbu88IWPZkuGkdfG+bcZfZvxN0ne1zWFGw+ppouX/seErVZ0agmg6atNrDEew+io47vLtJEAYxIgpHytKb4X3jrvhws3E3OFsqOGNiNvddPZOc2KD6/Z/+3cwmHbGdRcHe+1dFBQZ+d7PxkwPYWUe2AHUlConZZvC0YwIQcI/YJSbsmbCr3gzCx10Lx14RZCk0m4EwOdccxyYH+ps+NvDcUPdRMPZMP3j2PuXCzn0KtRRSC4PutwZVT6vpT1KOWVSXPMr01RaB1sNYCin55net2x9o26GvlijYImj/tlU7YPty837igvDPDiYmCaLjAS3xBGHII6JwpHhaoa2RFz7cz8vvm3UCt56Uyh+unmmqebY1mXRRMCmVY06FUVO7thQyxwcsBU9Qdk57Zo6VgdRcFXD9BLuPohxmcGsIshTsIHN7CYgg91FcGlz6MIw9I/BZ7ZaCNfONSwms9A1egGe3bw5TjcR2gQUPjlMuMq+xQYvrQkUhbUzgfXy6efW6jMjZg7aN/ezDWQrRMSaI/NZv4DdjOmcitbuPgtJco5xGiDYuMQv4bIHsDqUCcRlxHQlDHBGFI8Xrwt3ayK+WbWVGhvGnJ7qDZvHuJsidbmaq2VNg/k9g1HQjCsGLtsvXm5z+pFEBUXC3BGbvqUWm3o6dq99S1bG8QjDJuR3dR/asPDHbWt0b5D6KCzNAtzUZ901SkKUAZhZvvwcjQNHx4dcAhLqPwPT3+mVwys2Bc6Huo7Sg6rf297JFKnl0x7a2ZeCqMwLpcHbuR7jnBrvqIGCVBIvr2DPMuartMOm8rp8bymm3mtfu1kUIwhBAROFI0Bp8bZQeqqTF7WPBeGuQCvaxu5shaxJ8411YvNLM1otPNwO15XbC2wa73zIbtNi1hsAMhrafPyoKCubC/g9MeqarHo45CyZfaDZ3CSZ5dMB91FIDHz5i3idmm1z7lmrjp3c3dpy1O5xmhly/3wRc291HVhs7/hBMTKLZoCYUe/Yd6kYpPtVkSbXfHyIKtnvNfg8mJtJ00LiNgmkXhYbDrx4OFoXQkhdtTUbsgq2WSefBrRth4Z8CAe2eMO2ynrcVhEGMiMKR4DPxgtamer52+jgylOVisUXB7zez0JhEExOwB64ZV5kyDG/81FgM+1cb98yEcwPVQiFwr03hiaawXM1uc5ySD4ueDbiWbFJGB8pRv3Y7bH8NTv++CdImZBorw57JB1sKYAZpO75hD8rtojC5828QkxhwLdnxD+h+VW/wjNwZ4j6KijKpoQCJ1mKxtoaOlotNewxBHz7989grYdIF5n1LlVlxbf374W40fQ/+rWNTjJAc/8XOv1F3REXBrZvgm+/3/B5BGISIKPQSt9fPr/5l0iTTot1899wJAd+2LQr2YBmb1PFmpeDC+4zLY+MS2PG6cbOMPcMMQOHcRxBI67SDn12tsE3ONX1pqoQtS83ANv//mc9NyDQzZdsasfcFsHEmdhaFuMNYCjZpYyDKWhxmB5odYUQhODAeailAYFZvf77t7gn16zucgVpLoW6oUCacA1c+ad43V8Nj8+G342DVb4yAxSZ1tDaC3WS9Ja0wkBggCEMUEYVe8qc3d/DiB7sAyI3zmd3B7Hz5pgrjWnKHmUHbJGSYPRY2v2JW2xafbgbYuFQzSPk8Jh0zeCadP9sIybZXzXHw4BqM7Xv/4GGzJsBO3YSA+8gWntgwloK9yKvdUrAGyOwQiwQ6isKoqYHFbu2WQpiAawdRSOx83c5Asj+/xtpoOVQUlApYCM4eLBSLjjUL+JorTKppWwOsf8a4j2KSOv7WvbEOBGEYIqLQCzaW1vHgql1ccqwZ3By2RWBbCp4WM1O2/erhRAGMC6m+xPjL591hzsWlAjoQEwgeNGMSTS0he/ObhK5EwRo8P3zUrE0ILpDXLgq2pRAqComBBWj24J1aYGbi3VkK0XFw/u/gC9a2le3ZR2EG6+7cRxBkKVjtai1RCM0+goB7qqerhxMzzZoLv9d8p+Yq82/VyVIQURBGNiIKPcTn1/zvK5vITIzhu2db6ZOeZhM/aK0zgVow1oKd/x7qPrKZfIEZzGZdB4UnWG2twcjOHgp1r8wM2p+oS0shz7y2NcDJ3+q4iCoh06yotuv9hA5+9iCtHCZdFYyl8Z2Pws+e7fbOBJPZZM/y27OPwriPYhLN946KDm9J2OspUi1xqNlrfa/RndtGWwN5T8tUJ2QG0oFzZxhLqqnCWBDBv7VYCsIIR0Shh7ywdj+fltXz4wunkuzwBS54Wowo2Hnqf/siLPuhed+VpZCQAd9aAxcEFa+zffz2oB06k55xVeC9nccfim0pZIyDWV8M+UwrDmEHq0NjCvbAmJhlgqZgfPcpeeE/y7YU7NeoKCOM7dlHYQZ9ZaV/hrMSACZfBDe+aUpvgGUpqM6pt9B7SyEhK+DmGz3DvNaXWJZCkCiIpSCMcEQUusLvN4Fdvw+/X/PI27uZVZTGRTNGB0pQgxlo3I0BF0vF5oCbpytLAcxGLsEDpz1DbXcfhVgKsckw0cqb7yovPz7dzO4vuLdzm1CXTCdLwfq8rlxTocQEWQo2jpjuLQUwbpxwQWYwwpI/O/DMhjKzkC5chdP2mEIvLAWbXEsU/F7zuzpiAgsCRRSEEY6Uzu6KPavg+UVw2ndZmf9N9lW38IPPTzI7n/ncgXb2/sVh0za7EYVQ7Jm7nVIaLhC76NlAOmU4lIJLHgp/zR4UD3xiBsFQa8P+vK5cU6G0WwpBA3x0TCC+0lW5h8TsgDXRFcGz/+DyGsHYAebexBTAfPfgGElMkvndnAkmphJOgARhBCH/B3SFvbn8mod5avR55KbE8flplnsm2FKwRSHzGCg6xVgAG541545EFLpyH4FZSRzl6Pkzg7EL7FVsMQXqQgc/e3YezlUTjnZLIaifwauYuxKF2Td0XHUdjuDZf/CmOcH02lKwy4zndfyOtjUXkxBIcxWEEUxERUEptQC4H3AAj2mt7w65/gfgLOswAcjRWqdFsk89xvY/e10c3LmBRfPPwumwBo1wohCfAV9ZbrJabFHozn0Uij2Ttyt6duViOVKC3Sehi96CP+9oLIVgl1FX7qPg4nhdESx8wdtrBtPrmIL1/VPyQ1JjrbRbZ7zULRIEIhhTUEo5gAeB84CpwDVKqanBbbTW39Vaz9RazwT+CLwcqf70GjufHzjf8QFXzikMXPOFEwXLHZOYFdhYpquAajhiU8ygZO83cLhFWb0lNiWQIZUTxtXlPEL3UYeYgvX8KGcgWH205M0Mf7632UeJQZZCTFJAtGzhdiZKPEEQ6IEoKKW+o5TqIt2lW+YCO7XWu7XWbuAF4OJu2l8DPH8EnxMZWuvQKMpVDicnV5GfFjT4hLUUggycvJlm0OqNf1opU9ffXhXdG9dTT59vz5Zzpna+HtNL95EzJPsIAjPtviofnTSqa3eZ/Rm9DTTb22vaImH/zmmFpjChIIxwejJqjQLWKqU+Bp4AVmgdXOazS/KB4FrFpcCJ4RoqpcYAY4E3u7i+GFgMUFRUFK5J3+Oqwx+TwvbW0cx0BhW60zok0Gx9xeDA7ewbAuWqe0NiVmBVcV+7j8AMjE0HwwfFjzimEBJohr5xw3x3S/drBmwxiO6ppWB9L3uzoIRMk91kWwpXPEH7Pg6CMII5rKWgtf4xMAF4HLge2KGU+pVS6ghGvS5ZBLyotfaFu6i1fkRrPUdrPSc7u4eD1tHSWkeLI4ndOo/Uln0mRXXve/CrvIB1AGZDluj4wIIvMPV2zv157z/T3scA+t59BCbYHB0ffkZsD/K9TUmNCUlJhb6xFFLzu69D1G4p9DCmkF4MF90fWO/RbilYnxGTGBkhFoQhRo8cv5ZlcND68wLpwItKqd92c1sZEOSIp8A6F45FDCbXEYCrnlp/AtVxRUR5W02qaMkas1jNrskDJr6QXtw3PnR7Nhsdd+RZRt1RcAKMnx/+2bnHQtbEwKriw9EuCkFuLttP3x+7j0X3MiVVKZh9fcCis8XvaArgCcIw5LDuI6XULcCXgCrgMeAHWmuPUioK2AH8sItb1wITlFJjMWKwCLg2tJFSajJGZFYf0TeIENpVx0F3LImFk2E/Zt/eams1cGtIXX67GNzRYotCuDUKfcE5P+36Wu6xcNPanj+ru0BzV5lHfUlvU1JDaS/618exG0EY4vQkppABXKa17rBtldbar5TqMr9Qa+1VSt0ErMCkpD6htd6slPo5sE5rvdRqugh4oYdxin7D01RDlTed/PEzLFHYGSgRYW/W4rT2FMjoYoFVb7EHqt5kLQ0U3bqP+iG1s7eWQihJIZVgBUEAeiYKy4H2qbFSKgWYorX+QGu9tbsbtdbLgGUh534Scnxnj3vbj3hb6qjXBZw8ZTK8mwTVO6DGlMxu39YyPj0yojAUfNtpY+Dkm2DC5wLn2gPN/WEp9DL7KJSZ15mgc1d1pARhhNITR/jDQHBdgibr3LAm2l2PLyaZMVmJpizCvtWBdNHWGjPw2a6Hrkox9JakCLuP+pIoB3z+l6a8tk27pXCEs/fe0J59dBSWQnCRQUEQgJ6Jggp27Wit/Qzn8hgVW9G73yJGu0nLyDG1jiZ8Dg59GmjTWmtt3GIN3n3uPhoClkI42gPN/eE+OkpLQRCEsPREFHYrpW5WSjmtv1uA3ZHu2IDx0EmopxcCkJNj1TqasrBjG+0PiEJUdKD+/9GSaKWkDgVLIRwDEWjuD6tEEEYQPRGFbwCnYDKI7AVoiyPZqcHCmHxrc5ecKYH9EuyiaY5YszYhvbjvKmsmZAJq6FoK0f2YkpqYg9lroYfrKgRB6BGHHc201hWYDKERR06OtQ2kUnDCjbDt32b3LledcZHM/2lgl7W+wBFtFrDFD46agL2mr8tcdMf4+fDtDwNbeAqC0Cf0ZJ1CHPBVYBrQbqtrrb8SwX4NGDp9LMraiEYFZ6ac9A3z94fpRhQcsZA1vu87sOi5zhvVDxXa3Uf9EFNQCrInRv5zBGGE0RP30TNALvB54C3MyuQ+nB4PLry+oEoboVtWQtCG9REa+ArmdMzoGUr054pmQRAiQk9EYbzW+v8BzVrrvwAX0EVhu+GAzxWUfRsXxo3TLgoS4OyEbSnIbyMIQ5aeiIK9/2OdUmo6kArkdNN+SKM8rWxhHHrS+YHdyoKxRUE2ZOmMbSHIbyMIQ5aeiMIj1n4KPwaWAluA30S0VwOF1ji1i+3JJ6OueT584Ti7AJy4SDrTn4FmQRAiQreBZqvoXYPWuhZ4G+ijym+DFE8rUWjiErup42+LQn/k4g81+jPQLAhCROjWUrBWL3dVBXXY0dTcAEBicneiYMcURBQ60R5olpiCIAxVeuI+ekMp9X2lVKFSKsP+i3jPBoCDFVUApKaEyTqyEVHomv6skioIQkToyVLcq63Xbwed0wxDV9Kh6hrGA2mp3Swea3cfycDXif6skioIQkToyYrmPqr2NvipqqkFIDO9O1EQS6FL+rNKqiAIEaEnK5q/FO681vrpvu/OwFJbVwdAYlI3MYVYyT7qEnEfCcKQpyfuoxOC3scB84GPgWEnCnX1RhRUd1s0SvZR1zjEfSQIQ52euI++E3yslEoDXohYjwaQpsZ686a77TDFfdQ16WPMKvDMYwa6J4IgHCFHUvO5GRh2cQa/X9Pa3Gh2k+5uO0xZ0dw1aUVw+77DtxMEYdDSk5jCvzDZRmBSWKcCS3rycKXUAuB+zFD7mNb67jBtrgLutD7jE631tT3qeR9T0dhGjL/V9LS7/QzaVzRLMFUQhOFHTyyFe4Lee4F9WuvSw92klHIADwLnYjbnWauUWqq13hLUZgJwB3Cq1rpWKTVgNZX217QQT5s56G7ns3ZREEtBEIThR09EYT9wQGvtAlBKxSulirXWew9z31xgp9Z6t3XfC8DFmNpJNjcCD1plNOwNfQaE/TUtJKg2tHKgunMNJedCTDJkDLtlGoIgCD1a0fx3wB907LPOHY58oCTouNQ6F8xEYKJS6j2l1BrL3dQJpdRipdQ6pdS6ysrKHnx079lf00ICbcZKUKrrhvFpcEcJjJsXkX4IgiAMJD0RhWittds+sN73le8kGpgAzAOuAR61sps6oLV+RGs9R2s9Jzs7u48+uiMlNS1kx3pR3bmObLoTDUEQhCFMT0ShUim10D5QSl0MVPXgvjKgMOi4wDoXTCmwVGvt0VrvAT7DiES/s7+mhYwYT/dBZkEQhGFOT0ThG8D/KKX2K6X2Az8Cvt6D+9YCE5RSY5VSMcAizH4MwfwDYyWglMrCuJN297DvfUpJTQvp0Z7u01EFQRCGOT1ZvLYLOEkplWQdNx3mFvs+r1LqJmAFJtHzCa31ZqXUz4F1Wuul1rXPKaW2YGIVP9BaVx/hdzliWt0+KhrbSE70dL9wTRAEYZjTk3UKvwJ+q7Wus47Tgdu01j8+3L1a62XAspBzPwl6r4HvWX8DRmltCwCJqg1isgayK4IgCANKT9xH59mCAGClj54fuS71M7tXUfTkLMaog2adgsQUBEEYwfREFBxKqfZCP0qpeGD4FP5Z8zCxrkp+EL2EGG9zYHGaIAjCCKQni9eeBf6rlHoSUMD1wF8i2al+o6EcdvyHxuhMLmQNNAJZA5L8JAiCMCjoSaD5N0qpT4BzMPWJVgBjIt2xfuGT50H7uTfnLuZXPs1pn78ajg+7fYQgCMKIoKdVUg9hBOFKYA/wUsR61J/seQdGHct7zfmUFt7FaXPmDHSPBEEQBpQuYwpKqYlKqZ8qpbYBf8TUQFJa67O01n/qtx5GCr8fyj5CF5xASU0rRRkSYBYEQejOUtgGvANcqLXeCaCU+m6/9Ko/qNoObQ00Zs+k1eMTURAEQaD77KPLgAPASqXUo0qp+ZhA8/CgdC0A+xOmAYgoCIIg0I0oaK3/obVeBEwGVgK3AjlKqYeVUp/rrw5GjJIPIS6Nnd5cAApFFARBEA6/TkFr3ay1fk5rfRGmqN16TP2joc2hzTD6OPbXtgJQkB4/wEo8W+AAAAz7SURBVB0SBEEYeHqyeK0drXWtVcZ6fqQ61G+01kJiFvtrWshNiSPO6RjoHgmCIAw4vRKFYUVbI8SmsL+mhcIMsRIEQRBgRItCA8SlUFLTIvEEQRAEi5EpCh4X+Nx4nMkcbHBJ5pEgCILFyBSFtgYA6vzxaC3pqIIgCDYjUxRcRhQq3abYq4iCIAiCYWSKQls9AIfanICIgiAIgs3IFAXLUihpdRIbHUV28vDZHkIQBOFoGJmiYMUU9jc7yU+PR6nhU71DEAThaIioKCilFiiltiuldiqlbg9z/XqlVKVSaoP197VI9qcdy1LY1xRFfpqsURAEQbDp6X4KvUYp5QAeBM4FSoG1SqmlWustIU3/prW+KVL9CEtbIwA766M4qVBEQRAEwSaSlsJcYKfWerfW2g28AFwcwc/rObb7qCVaLAVBEIQgIikK+UBJ0HGpdS6Uy5VSG5VSLyqlCsM9SCm1WCm1Tim1rrKy8uh75mrAH52ADwf5UghPEAShnYEONP8LKNZazwBeB/4SrpFVhG+O1npOdnb20X9qWz0eZzIAeakiCoIgCDaRFIUyIHjmX2Cda0drXa21brMOHwNmR7A/AVwNuByJAGIpCIIgBBFJUVgLTFBKjVVKxQCLgKXBDZRSo4MOFwJbI9ifAG0NNJOAI0qRmxLXLx8pCIIwFIhY9pHW2quUuglYATiAJ7TWm5VSPwfWaa2XAjcrpRYCXqAGuD5S/emAq4F6nUBuShzRjoH2oAmCIAweIiYKAFrrZcCykHM/CXp/B3BHJPsQlrYGan15knkkCIIQwsicJrc1Uu2NZVSquI4EQRCCGZmi4GqgyhNLdpLUPBIEQQhm5ImCzwveVmq9cWQlxwx0bwRBEAYVI08UPC0AtCCWgiAIQigjUBRaAXARIyWzBUEQQhh5ouANiEKWWAqCIAgdGHmiYFkKrTqWHLEUBEEQOjBiRaFNOclIlECzIAhCMCNWFKJjE2U1syAIQggjb1S0YgrxCUkD3BFBEITBx8gTBctSSEhMHuCOCIIgDD5GoCi4AEhMFEtBEAQhlBEnCtpavJacnDLAPREEQRh8jDhRcLuaAUhOEktBEAQhlBEnCq6WJkAsBUEQhHCMOFFoaxVLQRAEoStGpCi4tJP0JNlLQRAEIZQRJwretmZcxJCe4BzorgiCIAw6RqAotNBKLGkJUuJCEAQhlIiKglJqgVJqu1Jqp1Lq9m7aXa6U0kqpOZHsD4C/rYVWHUNavFgKgiAIoURMFJRSDuBB4DxgKnCNUmpqmHbJwC3AB5HqSzDa04onKlbqHgmCIIQhkiPjXGCn1nq31toNvABcHKbdL4DfAK4I9iWApxVPlASZBUEQwhFJUcgHSoKOS61z7SiljgcKtdavRrAfHVA+F36H7KMgCIIQjgHzoSilooB7gdt60HaxUmqdUmpdZWXlUX2uw9uK3xF/VM8QBEEYrkRSFMqAwqDjAuucTTIwHVillNoLnAQsDRds1lo/orWeo7Wek52dfVSdiva3gVPcR4IgCOGIpCisBSYopcYqpWKARcBS+6LWul5rnaW1LtZaFwNrgIVa63UR7BNOfxs4EyL5EYIgCEOWiImC1toL3ASsALYCS7TWm5VSP1dKLYzU53aH2+snhjYcMeI+EgRBCEd0JB+utV4GLAs595Mu2s6LZF8A6lraSMCNI1YsBUEQhHCMnGT9Dx4h88FJJCkX0bGJA90bQRCEQcnIEYWEDBxt9QA440QUBEEQwjFyRCF3RvtbZ5y4jwRBEMIxckQh85j2t+I+EgRBCM/IEYX/397dxVhxl3Ec//7cspSljZZCgPBSoN0bCIqE1MaQXjQGAS+o6UVpmkhMk8amGIzRFFNjGuONJDYGJSY0YtBUiYk2clG1FY0vsbZdzfJmg1CKaQktS1WqbN1ll8eL+TNOtnvOwu7Omd0zv09ycmb+M+w+D//d85xnZnbO+zr4b+ccwJ2CmVkj9SkKwMWu2wDo9NVHZmajqlVRePvGpQDcOPh2xZGYmU1NtSoKPfPuA+CGFesrjsTMbGqqVVF4rfMOVseP0YLVVYdiZjYl1aoo9A8M0zWzo+owzMymrFoVhUuDQ8zuLPXOHmZm01qtikL/oDsFM7NmalYUhuia4U7BzKyRmhUFdwpmZs3UqihcGvA5BTOzZmpVFPoHh+nqdKdgZtZIrYrCpYEhZs90p2Bm1khtikJEuFMwMxtDbYrC4PAVhq6EOwUzsyZKLQqSNko6IemUpJ2jbP+MpKOSeiX9QdLKsmJ5d3AYgFkz3CmYmTVSWlGQ1AHsATYBK4EHRnnR/2FErI6INcAu4Mmy4rmUisJsX5JqZtZQmZ3CncCpiDgdEYPAAWBLcYeIeKewOhuIsoLpHxgCoMuXpJqZNVTmK+Qi4PXC+hvAR0buJOlR4PNAJ3BPWcG4UzAzG1vlJ5ojYk9E3A48Bnx5tH0kPSypR1JPX1/fuL6POwUzs7GVWRTOAksK64vTWCMHgHtH2xAReyNiXUSsmzdv3riCyTsFFwUzs4bKLAovA92SlkvqBLYCB4s7SOourH4COFlWMP2DWacwy3+nYGbWUGlvmyNiSNJ24JdAB7AvIo5L+irQExEHge2SPgZcBv4JbCsrnn6fUzAzG1Opx1Ii4lng2RFjXyks7yjz+xdd8jkFM7MxVX6iuVWWzuli46oFvs2FmVkTtXnbvGHVAjasWlB1GGZmU1ptOgUzMxubi4KZmeVcFMzMLOeiYGZmORcFMzPLuSiYmVnORcHMzHIuCmZmllNEaZ9rUwpJfcDfx/nP5wIXJjGc6aCOOUM983bO9TDenG+LiDFvMz3tisJESOqJiHVVx9FKdcwZ6pm3c66HsnP24SMzM8u5KJiZWa5uRWFv1QFUoI45Qz3zds71UGrOtTqnYGZmzdWtUzAzsyZcFMzMLFeboiBpo6QTkk5J2ll1PGWRdEbSUUm9knrS2BxJz0s6mZ5vqTrOiZC0T9J5SccKY6PmqMzuNO9HJK2tLvLxa5DzE5LOprnulbS5sO1LKecTkj5eTdQTI2mJpN9I+quk45J2pPG2nesmObduriOi7R9AB/AqsALoBA4DK6uOq6RczwBzR4ztAnam5Z3A16uOc4I53g2sBY6NlSOwGfg5IOAu4MWq45/EnJ8AvjDKvivTz/hMYHn62e+oOodx5LwQWJuWbwb+lnJr27luknPL5rouncKdwKmIOB0Rg8ABYEvFMbXSFmB/Wt4P3FthLBMWEb8D/jFiuFGOW4DvR+ZPwAckLWxNpJOnQc6NbAEORMRARLwGnCL7HZhWIuJcRPwlLf8beAVYRBvPdZOcG5n0ua5LUVgEvF5Yf4Pm/9HTWQDPSfqzpIfT2PyIOJeW3wTmVxNaqRrl2O5zvz0dKtlXOCzYdjlLWgZ8GHiRmsz1iJyhRXNdl6JQJ+sjYi2wCXhU0t3FjZH1nG19HXIdcky+A9wOrAHOAd+oNpxySLoJ+AnwuYh4p7itXed6lJxbNtd1KQpngSWF9cVprO1ExNn0fB54hqyVfOtqG52ez1cXYWka5di2cx8Rb0XEcERcAZ7i/4cN2iZnSTPIXhyfjoifpuG2nuvRcm7lXNelKLwMdEtaLqkT2AocrDimSSdptqSbry4DG4BjZLluS7ttA35WTYSlapTjQeBT6cqUu4CLhUMP09qI4+WfJJtryHLeKmmmpOVAN/BSq+ObKEkCvgu8EhFPFja17Vw3yrmlc1312fYWntXfTHYm/1Xg8arjKSnHFWRXIhwGjl/NE7gVOAScBH4FzKk61gnm+SOyFvoy2THUhxrlSHYlyp4070eBdVXHP4k5/yDldCS9OCws7P94yvkEsKnq+MeZ83qyQ0NHgN702NzOc90k55bNtW9zYWZmubocPjIzs2vgomBmZjkXBTMzy7komJlZzkXBzMxyLgpmI0gaLtyNsncy76oraVnxTqdmU80NVQdgNgW9GxFrqg7CrAruFMyuUfqsil3p8ypeknRHGl8m6dfpZmWHJC1N4/MlPSPpcHp8NH2pDklPpfvlPydpVmVJmY3gomD2XrNGHD66v7DtYkSsBr4NfDONfQvYHxEfBJ4Gdqfx3cBvI+JDZJ+FcDyNdwN7ImIV8C/gvpLzMbtm/otmsxEk/Scibhpl/AxwT0ScTjctezMibpV0gey2A5fT+LmImCupD1gcEQOFr7EMeD4iutP6Y8CMiPha+ZmZjc2dgtn1iQbL12OgsDyMz+3ZFOKiYHZ97i88v5CW/0h2512AB4Hfp+VDwCMAkjokvb9VQZqNl9+hmL3XLEm9hfVfRMTVy1JvkXSE7N3+A2nss8D3JH0R6AM+ncZ3AHslPUTWETxCdqdTsynL5xTMrlE6p7AuIi5UHYtZWXz4yMzMcu4UzMws507BzMxyLgpmZpZzUTAzs5yLgpmZ5VwUzMws9z/U6u3SjO6ymAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(better_log_df['acc'])\n",
    "plt.plot(better_log_df['val_acc'])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-AbFmueZT92"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNST_CIFAR10_BC_L16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
